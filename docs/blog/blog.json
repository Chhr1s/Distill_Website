[
  {
    "path": "blog/apa-2021/",
    "title": "APA 2021 Supplemental Material",
    "description": "Supplmental Material for the Application of Generalized Linear Mixed-Effects Model Regression Trees with Multi-Modal Data poster presented at APA 2021, co-authored with Matthew C. Graham, Edwin M. Zorilla, Idalis Villanueva Alarcon, Jenefer Husman, & Keith Zvoch",
    "author": [
      {
        "name": "Christopher M. Loan",
        "url": {}
      }
    ],
    "date": "2021-08-06",
    "categories": [
      "Collaboration",
      "Conference",
      "Machine Learning",
      "Multi-Level Modeling",
      "MLM",
      "Plotting",
      "Simulated Data",
      "Tutorial"
    ],
    "contents": "\n\nContents\nBackground\nload libraries\nsimulating data\n\nFitting Generalized Linear Mixed Effects Model Regression (GLMM) Trees via {glmertree}\nModel 1: Fully Exploratory\nModel 2: Modify hyperparameters\nModel 3: add fixed effects\n\nVisualizing the Data\nGoing beyond significance\nReferences & Recommended Reading\n\nBackground\nThe purpose of these supplemental materials is to walk users through a few models and technical details associated with the {glmertree} package and provide references/supplemental reading for our poster. Refer to the Table of Contents (or scroll down) if you’re just here for references & recommended reading.\nFor readability, code is included but can be shown/hidden by clicking show code.\nload libraries\n\n\nshow code\n\nlibrary(glmertree)\nlibrary(tidyverse)\nlibrary(ggparty)\n\n\n\nsimulating data\nFor data privacy, I’m using simulated data to walk you through these models. If you don’t care about the simulation and just want to see model specification, keep reading; you’re in the right place! If you’re interested in knowing how I simulated the data, check out this blog post of mine.\nI made a complex interaction term that would not be easily detected by a multilevel model (MLM), and showed how you would have to specify the fixed effects to appropriately approximate the data. This basically resulted in a 3-way interaction that would be highly unlikely to be theorized in multimodal approached, barring strong theoretical evidence or equally strong analyst capability (e.g., to draw associations from graphic exploration of the data).\nWe have 15 level 1 units for each level 2 unit and 71 level 2 units (labeled with id). The level 1 intercept of outcome is 4.25 with a main effect of 1.25 for x, 2.00 for z. The function also makes a nuisance variable w, which does not contribute to outcome at all.\n\n\nshow code\n\nsim_dat <-\n  simulate_nonlinear_intx(\n    n = 15, ## number level 1 units at each level 2\n    j = 71, ## number level 2 units\n    intercept_lv1 = 4.25, ## intercept at level 1\n    main_x = 1.25, ## main effect of x\n    main_z = 2.00, ## main effect of z\n    interaction1 = -4, ## interaction of x and z when product < 0\n    interaction2 = 4, ## interaction of x and z when product >= 0\n    residual_var_sd_lv1 = 2.00, ## standard deviation of residuals at level 1\n    random_int_mean_lv2 = 0, ## mean of random intercept at level 2\n    random_int_sd_lv2 = 1.00, ## standard deviation of random intercept at level 2,\n    start_seed = 123 ## ensure you can reproduce \n  )\n\n\n\nGLMM trees should be able to find heterogeneity in the association of outcome and x (or z, it doesn’t matter) and explain differential effects of z and x without analyst knowledge of a complex higher-order interaction (i.e., n-way interactions when n > 2).\nFitting Generalized Linear Mixed Effects Model Regression (GLMM) Trees via {glmertree}\nSpecifying models with {glmertree} is similar to multilevel models (MLM) with {lme4}. However, rather than adding random effects with {lme4}, you specify the model as follows: forumla = outcome ~ fixed effects | random effects | splitting variables.\nA second consideration when fitting GLMM trees is the model’s use of parameter instability tests; therefore, we need to account for cluster covariances in this process, so you want to specify the id variable to the cluster = id argument as well as in the specified formula. As far as I know, the current implementation only allows a 2 level structure, even if the MLM\nModel 1: Fully Exploratory\nLet’s run a very exploratory model here. This is everything but the kitchen sink: an intercept-only model, with random intercept for each level 2 unit, and allowing splits to be made by any of the variables in the data.\n\n\nshow code\n\ntree_1 <- \n  lmertree(\n    data = sim_dat, \n    formula = \n      outcome ~ 1 | (1 | id) | x + z + w, \n    cluster = id\n  )\n\n\n\nLet’s look at plotted results first. Here are the random effects, presented as a caterpillar plot. These random effects will be used as offsets in the linear models fit within each node, if you interpret the model from the one-level with multiple node framework (i.e., from summary(tree_1$tree))\n\n\nshow code\n\nplot(tree_1, which = 'ranef')\n\n\n$id\n\n\n\n\nshow code\n\ncustom_boxplot <- \n  function(\n    glmertree_object,\n    write_labels = F\n  ){\n    format <- list(geom_edge(size = 1.2),\n    geom_node_label(\n      aes(label = splitvar),\n      ids = \"inner\"),\n    geom_node_plot(\n        gglist = list(\n          geom_boxplot(\n            aes(y = outcome), \n            size = 2),\n          theme_minimal(base_size = 12),\n          theme(\n            axis.title.x = element_blank(),\n            axis.text.x = element_blank(),\n            axis.line.y = element_line(size = 1.5),\n            panel.grid.minor.x = element_blank()\n            )),\n        shared_axis_labels = TRUE),\n    geom_node_label(\n      line_list = \n        list(\n          aes(label = splitvar),\n          aes(label = paste('N = ' , nodesize)),\n          aes(label = if_else(p.value < 0.001, \"p < 0.001\", paste0('p = ', round(p.value, 3))))),\n      line_gpar = list(\n        list(size = 15),\n        list(size = 12),\n        list(size = 12)),\n      ids = \"inner\"),\n    geom_node_label(\n      aes(\n        label = paste0(\"N = \", nodesize)\n        ),\n      ids = \"terminal\", \n      line_gpar = list(list(size = 15))\n      )\n    )\n    if (write_labels == T) {\n      return(\n        ggparty(glmertree_object$tree[1]) + geom_edge_label() + format\n        )\n    }\n    else {return(ggparty(glmertree_object$tree[1]) + format)}\n  }\n\n\n\nHere’s a printed (instead of plotted) tree diagram. This diagram is too large to print easily, so it’s obviously more complex than we want:\n\n\nshow code\n\ntree_1$tree\n\n\nLinear model tree\n\nModel formula:\noutcome ~ 1 | x + z + w\n\nFitted party:\n[1] root\n|   [2] z <= 1.05788\n|   |   [3] x <= 0.92647\n|   |   |   [4] z <= 0.37947\n|   |   |   |   [5] x <= 0.66556\n|   |   |   |   |   [6] x <= -1.81567: n = 10\n|   |   |   |   |       (Intercept) \n|   |   |   |   |          7.955253 \n|   |   |   |   |   [7] x > -1.81567\n|   |   |   |   |   |   [8] z <= 0.1324\n|   |   |   |   |   |   |   [9] x <= -0.89285\n|   |   |   |   |   |   |   |   [10] z <= -0.50041\n|   |   |   |   |   |   |   |   |   [11] z <= -1.26949: n = 12\n|   |   |   |   |   |   |   |   |       (Intercept) \n|   |   |   |   |   |   |   |   |           8.72907 \n|   |   |   |   |   |   |   |   |   [12] z > -1.26949: n = 33\n|   |   |   |   |   |   |   |   |       (Intercept) \n|   |   |   |   |   |   |   |   |          5.658304 \n|   |   |   |   |   |   |   |   [13] z > -0.50041: n = 35\n|   |   |   |   |   |   |   |       (Intercept) \n|   |   |   |   |   |   |   |            3.1839 \n|   |   |   |   |   |   |   [14] x > -0.89285\n|   |   |   |   |   |   |   |   [15] x <= 0.23029\n|   |   |   |   |   |   |   |   |   [16] z <= -0.89841\n|   |   |   |   |   |   |   |   |   |   [17] x <= -0.26504: n = 42\n|   |   |   |   |   |   |   |   |   |       (Intercept) \n|   |   |   |   |   |   |   |   |   |          4.032858 \n|   |   |   |   |   |   |   |   |   |   [18] x > -0.26504: n = 47\n|   |   |   |   |   |   |   |   |   |       (Intercept) \n|   |   |   |   |   |   |   |   |   |          1.914251 \n|   |   |   |   |   |   |   |   |   [19] z > -0.89841: n = 155\n|   |   |   |   |   |   |   |   |       (Intercept) \n|   |   |   |   |   |   |   |   |          4.057118 \n|   |   |   |   |   |   |   |   [20] x > 0.23029: n = 99\n|   |   |   |   |   |   |   |       (Intercept) \n|   |   |   |   |   |   |   |          4.654456 \n|   |   |   |   |   |   [21] z > 0.1324: n = 77\n|   |   |   |   |   |       (Intercept) \n|   |   |   |   |   |          5.229926 \n|   |   |   |   [22] x > 0.66556: n = 72\n|   |   |   |       (Intercept) \n|   |   |   |          6.074059 \n|   |   |   [23] z > 0.37947: n = 164\n|   |   |       (Intercept) \n|   |   |          6.806958 \n|   |   [24] x > 0.92647\n|   |   |   [25] z <= 0.56022\n|   |   |   |   [26] z <= -0.76973\n|   |   |   |   |   [27] x <= 1.33385: n = 19\n|   |   |   |   |       (Intercept) \n|   |   |   |   |           8.61226 \n|   |   |   |   |   [28] x > 1.33385: n = 23\n|   |   |   |   |       (Intercept) \n|   |   |   |   |          12.99779 \n|   |   |   |   [29] z > -0.76973\n|   |   |   |   |   [30] z <= 0.31077\n|   |   |   |   |   |   [31] z <= -0.31704: n = 33\n|   |   |   |   |   |       (Intercept) \n|   |   |   |   |   |          8.141308 \n|   |   |   |   |   |   [32] z > -0.31704: n = 39\n|   |   |   |   |   |       (Intercept) \n|   |   |   |   |   |          6.283608 \n|   |   |   |   |   [33] z > 0.31077: n = 21\n|   |   |   |   |       (Intercept) \n|   |   |   |   |          9.217174 \n|   |   |   [34] z > 0.56022: n = 26\n|   |   |       (Intercept) \n|   |   |          12.82893 \n|   [35] z > 1.05788\n|   |   [36] x <= 1.13431\n|   |   |   [37] z <= 1.95389\n|   |   |   |   [38] x <= -0.99028: n = 16\n|   |   |   |       (Intercept) \n|   |   |   |          14.03631 \n|   |   |   |   [39] x > -0.99028\n|   |   |   |   |   [40] x <= 0.53007: n = 70\n|   |   |   |   |       (Intercept) \n|   |   |   |   |          9.506037 \n|   |   |   |   |   [41] x > 0.53007: n = 30\n|   |   |   |   |       (Intercept) \n|   |   |   |   |          12.58756 \n|   |   |   [42] z > 1.95389: n = 17\n|   |   |       (Intercept) \n|   |   |          15.56201 \n|   |   [43] x > 1.13431\n|   |   |   [44] z <= 1.58438: n = 15\n|   |   |       (Intercept) \n|   |   |          16.82666 \n|   |   |   [45] z > 1.58438: n = 10\n|   |   |       (Intercept) \n|   |   |          23.86421 \n\nNumber of inner nodes:    22\nNumber of terminal nodes: 23\nNumber of parameters per node: 1\nObjective function (residual sum of squares): 5187.452\n\nIt looks like our model has identified too many splits for this to extract meaningful interpretations We gain knowledge that x and z are the variables that contribute to the outcome. The model is never presenting w as a variable influencing splitting when there is so much more heterogeneity in levels of z and x. We can prevent the model from building such a large tree with the maxdepth parameter. We’ll get broader strokes of the process, but will be less likely to overfit the data.\nNOTE: For consistency when knitting the document, I actually wrote a custom function (above) to make {ggparty} plotting easy across the project, but you can also just use plot(tree_1, which = 'tree').\nThere are two ways we could begin working to make results which can be interpreted. One is to just tune hyperparameters. The other is to add fixed effects, which we’ll do later.\nModel 2: Modify hyperparameters\nHyperparameters are parameters that can be specified by the researcher; these parameters influence the estimation of the parameters in the model. These are common for machine learning, and those relevant to GLMM trees are any that are relevant to decision trees or parameter instability tests (see {partykit}).\nI’ll point you towards a few and their defaults.\nbonferroni = TRUE: Bonferroni corrects p-values. Recommended to leave on.\nalpha = 0.05: p-value indicated significant parameter stability (e.g., alpha = 0.001).\nminsize = NULL: the minimum cases allowed in the terminal node. The larger you make this, the more of the sample has to be in a node for you to trust there is generalize-able heterogeneity and not just noise in your data (e.g., minsize = 0.10*nrow(your_data_set))\ntrim = NULL: this specifies the percentage (if < 0) or number (if > 0) of outliers to be be omitted from the parameter instability tests. This is done to prevent undue influence of outliers, and does not influence group assignment/splitting; it is just used in testing.\nmaxdepth = Inf: the depth of the tree that’s allowed to grow. This functionally decreases the level of interactions you allow the data to find.\nThese should be tuned carefully and resulting models are ideally discussed with content experts and compared to prior research for validity.\nLet’s re-run this with some more stringent hyperparameters and see how the model explains relation of x, z, and outcome. Since this grew such a deep tree, that’s the first hyperparameter I’ll tune.\n\n\nshow code\n\ntree_2 <- \n  lmertree(\n    data = sim_dat, \n    formula = \n      outcome ~ 1 | (1 | id) | w + z + x, \n    cluster = id, \n    maxdepth = 3, \n  )\n\n\n\n\n\nshow code\n\ncustom_boxplot(tree_2, write_labels = T)\n\n\n\n\nModel 3: add fixed effects\nThe p < 0.001 on the nodes indicate the parameter instability test is significant and the 2 models explain more variance in the data than 1 model (repeated at subsequent nodes, too). The value for the test can be found via partykit::sctest.modelparty(glmertree_model$tree, type = c(\"chow\"), asymptotic = F) when your model name is glmertree_model. If these were barely p < 0.05, I would likely tune this next. Also, this model looks fairly interprettable.\nThis is saying z being ~1 standard deviation above its mean, the outcome is highest. When it’s lower than that, it’s lower. Within these cases, outcome appears to be higher when x is higher. This suggests an interaction of x and z. We could now see what the relationship of x to the outcome is for different values of z, by adding it as a fixed effect.\n\n\nshow code\n\ntree_3 <- \n  lmertree(\n    data = sim_dat, \n    formula = \n      outcome ~ x | (1 | id) | w + z + x, \n    cluster = id, \n    maxdepth = 3, \n    trim = 0.1\n  )\n\n\n\n\n\nshow code\n\ncustom_scatter_lm <-\n  function(\n    glmertree_object\n  ){\n    ggparty(glmertree_object$tree[1]) +\n    geom_edge(size = 1.2) +\n    geom_edge_label() +\n    geom_node_label(\n      aes(label = splitvar),\n      ids = \"inner\") +\n  geom_node_plot(\n      gglist = list(\n        geom_point(\n          aes(y = outcome, x = x), \n          size = 2),\n        geom_smooth(method = 'lm', aes(y = outcome, x = x)),\n        theme_minimal(base_size = 30),\n        theme(\n          axis.line.y = element_line(size = 1.5),\n          panel.grid.minor.y = element_blank()\n          )),\n      shared_axis_labels = TRUE,\n      scales = 'free_x')+\n  geom_node_label(\n    line_list = \n      list(\n        aes(label = splitvar),\n        aes(label = paste('N = ' , nodesize)),\n        aes(label = if_else(p.value < 0.001, \"p < 0.001\", paste0('p = ', round(p.value, 3))))),\n    line_gpar = list(\n      list(size = 20),\n      list(size = 20),\n      list(size = 20)),\n    ids = \"inner\") +\n  geom_node_label(\n    aes(\n      label = paste0(\"N = \", nodesize)\n      ),\n    ids = \"terminal\", \n    line_gpar = list(list(size = 20))\n  ) +\n      labs(caption = 'note x axes are free') +\n      theme(plot.caption = element_text(size = 20))\n  }\n\n\n\n\n\nshow code\n\ncustom_scatter_lm(tree_3)\n\n\n\n\nWe now see a more nuanced description of the relationship of x to outcome at different levels of z, and the subgroups are slightly different than in tree_2. When z is large, we see larger average outcome (i.e., intercept) than when z is lower. On both halves of the plot, it looks like the relationship of x and outcome is negative when x < 0, but is positive when x > 0. Let’s put these onto one plot and see what it looks like.\nVisualizing the Data\n\n\nshow code\n\nfigure_1a <-   \n  sim_dat %>% \n  mutate(node = factor(as.vector(predict(tree_3, type = 'node')))) %>% \n  ggplot(aes(x = x, y = outcome, group = node, color = node)) + \n  geom_point(alpha = 0.6) +\n  geom_smooth(method = 'lm') +\n  theme_minimal() +\n  project_theme +\n  labs(\n    title = 'Model fit by glmertree',\n    ) +\n  theme(plot.subtitle = element_text(size = 15)) \nfigure_1a\n\n\n\n\nGoing beyond significance\nThe plot above shows that we created 4 linear models that explain the data very well. We also see, though, that the 2 models on the left overlap with one another, as do the 2 models on the right. This is where we would call in researchers that know the field. We would discuss the evidence in the data, compared to the theoretical possibilities of 4 versus 2 groups.\nThese 4 groups may be phenomena observed or the way the model is formed may influence the fact that we have 2 parallel lines. You see, since the tree structure is dependent upon the first split, the parameter instability tests do not go across branches to see if groups could be recombined to increase model fit\nIf an analyst were working with a theorist, they should discuss the implications of these 4 groups and find some way to demonstrate validity of 4 over 2 (as Occum’s Razor would suggest 2 is more likely than 4). If an analyst were working alone, I would suggests following up this with one final parameter instability test, comparing models with individual negative slopes to a single negative slope. Then, repeating this for 1 vs. 2 positive slopes.\nVisually, two groups would look like this\n\n\nshow code\n\nsim_dat %>% \n  mutate(node = factor(as.vector(predict(tree_3, type = 'node'))),\n         dichot_node = factor(if_else(node %in% c(4,7), 1, 0))\n         ) %>% \n  ggplot(aes(x = x, y = outcome, group = dichot_node, color = dichot_node)) + \n  geom_point() +\n  geom_smooth(method = 'lm') +\n  theme_minimal() +\n  project_theme +\n  labs(\n    title = 'Collapsing Nodes',\n    ) +\n  theme(plot.subtitle = element_text(size = 15)) \n\n\n\n\nThis looks like a reasonable model, which could explain not only the sample, but the population as it does for us (based on its similarity to the method of simulation). Random forest modifications could be useful to preventing such overfitting, but that will have to wait for another article.\nReferences & Recommended Reading\nBates, D., Maechler, M., Bolker, B., & Walker, S. (2015). Fitting Linear Mixed-Effects Models Using lme4. Journal of Statistical Software, 67(1), 1-48. doi:10.18637/jss.v067.i01.\nFokkema, M., Edbrooke-Childs, J., & Wolpert, M. (2020). Generalized linear mixed-model (GLMM) trees: A flexible decision-tree method for multilevel and longitudinal data. Psychotherapy Research, 31(3), 329-341.\nFokkema, M., Smits, N., Zeileis, A., Hothorn, T., & Kelderman, H. (2018). Detecting treatment-subgroup interactions in clustered data with generalized linear mixed-effects model trees. Behavior research methods, 50(5), 2016-2034.\nHothorn, T. & Zeileis, A. (2015). partykit: A Modular Toolkit for Recursive Partytioning in R. Journal of Machine Learning Research, 16, 3905-3909.\nHothorn, T., Hornik, K., & Zeileis, A. (2006). Unbiased recursive partitioning: A conditional inference framework. Journal of Computational and Graphical statistics, 15(3), 651-674.\nHulleman, C. S., Godes, O., Hendricks, B. L., & Harackiewicz, J. M. (2010). Enhancing interest and performance with a utility value intervention. Journal of educational psychology, 102(4), 880.\nBorkovec, M. & Madin, N. (2019). ggparty: ‘ggplot’ Visualizations for the ‘partykit’ Package. R package version 1.0.0.\nVillanueva, I., Husman, J., Christensen, D., Youmans, K., Khan, M.T., Vicioso, P., Lampkins, S. and Graham, M.C., 2019. A Cross-Disciplinary and Multi-Modal Experimental Design for Studying Near-Real-Time Authentic Examination Experiences. JoVE (Journal of Visualized Experiments), (151), p.e60037.\nWickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686,\nZeileis, A. (2001). strucchange: Testing for structural change in linear regression relationships. R News, 1(3), 8-11.\nZeileis, A., Hothorn, T., & Hornik, K. (2008). Model-based recursive partitioning. Journal of Computational and Graphical Statistics, 17(2), 492-514.\n\n\n\n",
    "preview": "blog/apa-2021/apa-2021_files/figure-html5/plot_ranef_1-1.png",
    "last_modified": "2021-08-09T11:00:10-07:00",
    "input_file": "apa-2021.utf8.md"
  },
  {
    "path": "blog/simulating-2-level-data-for-apa-2021-supplemental/",
    "title": "Simulating Non-Linear Interactions",
    "description": "Simulating 2-level data for APA 2021 Supplemental, so others can follow along.",
    "author": [
      {
        "name": "Christopher Loan",
        "url": {}
      }
    ],
    "date": "2021-08-03",
    "categories": [
      "Conference",
      "Multi-Level Modeling",
      "MLM",
      "Plotting",
      "Simulated Data",
      "Tutorial"
    ],
    "contents": "\n\nContents\nDisclaimer\nGoal\nFunction\nSimulating data\nPlotting the Simulated Data\nFitting multilevel model to the nonlinear interaction\nExploring the Data with the {glmertree} package\n\n\n\nshow code\n\nlibrary(tidyverse)\nlibrary(glmertree)\n\n\n\nThis post is meant for two audiences:\nanyone who wants to follow along with my APA 2021 Supplemental Material\nsomeone looking to simulate complex interaction with multilevel modeling.\nDisclaimer\nMy code is available for use and alteration, but please give credit via citations. Anyways, let’s get into it! If you’re looking for more detail regarding the simulated data or would like to simulate your own data with a simple linear interaction, see this prior post which looks more deeply at distribtions of simulated data.\nGoal\nSimulate data to use in my APA 2021 Supplemental Material Post.\nFunction\nSimulating data\nBy modifying the interaction in the previous data simulation function with an if_else() statement, we can make a more complex interaction term that would not be easily detected by traditional multilevel models (MLMs). I am coding the interaction’s influence as conditional on it’s value, more specifically the interaction’s effect is: if_else(z[[i]]*x[[i]] < 0, interaction1*z[[i]]*x[[i]], interaction2*z[[i]]*x[[i]]). This is one type of circumstance where MLMs may struggle without substantial theoretical understanding to guide exploration of data.\nSo that is a simple simulation with a complex interaction term. We have 15 level 1 units for each level 2 unit and 71 level 2 units (labeled with id). The level 1 intercept of outcome is 4.25 with a main effect of 1.25 for x, 2.00 for z. The function also makes a nuisance variable w, which does not contribute to outcome at all.\n\n\nshow code\n\nnonlinear_dat <-\n  simulate_nonlinear_intx(\n    n = 15, ## number level 1 units at each level 2\n    j = 71, ## number level 2 units\n    intercept_lv1 = 4.25, ## intercept at level 1\n    main_x = 1.25, ## main effect of x\n    main_z = 2.00, ## main effect of z\n    interaction1 = -4, ## interaction of x and z when product < 0\n    interaction2 = 4, ## interaction of x and z when product >= 0\n    residual_var_sd_lv1 = 2.00, ## standard deviation of residuals at level 1\n    random_int_mean_lv2 = 0, ## mean of random intercept at level 2\n    random_int_sd_lv2 = 1.00, ## standard deviation of random intercept at level 2,\n    start_seed = 123 ## ensure you can reproduce \n  )\n\n\n\nPlotting the Simulated Data\n\n\nshow code\n\nfigure_1a <-   \n  nonlinear_dat %>% \n  ggplot(aes(x = x, y = outcome)) + \n  geom_point() +\n  geom_smooth(method = 'lm') +\n  theme_minimal() +\n  project_theme +\n  labs(\n    title = 'Figure 1. Association of Outcome & Each Covariate (non-linear intx)',\n    subtitle = '1a. Association of Outcome & x'\n    ) +\n  theme(plot.subtitle = element_text(size = 15))\n\nfigure_1b <-   \n  nonlinear_dat %>%\n  ggplot(aes(x = z, y = outcome)) + \n  geom_point() +\n  geom_smooth(method = 'lm') +\n  theme_minimal() +\n  project_theme +\n  labs(subtitle = '1b. Association of Outcome & z') +\n  theme(plot.subtitle = element_text(size = 15))\n\nfigure_1c <- \n  nonlinear_dat %>% \n  ggplot(aes(x = x*z, y = outcome)) + \n  geom_point() +\n  geom_smooth(method = 'lm', aes(group = x*z < 0)) +\n  theme_minimal() +\n  project_theme +\n  labs(subtitle = '1c. Association of Outcome & Interaction') +\n  theme(plot.subtitle = element_text(size = 15))\n\nggpubr::ggarrange(figure_1a, figure_1b, figure_1c, ncol = 1)\n\n\n\n\nFitting multilevel model to the nonlinear interaction\nWhen we fit an MLM to the data, it is unable to find a significant effect. You see this from the small t-values in the MLM results.\n\n\nshow code\n\nmisspecified_lmer <- \n  lmer(\n    data = nonlinear_dat, \n    formula = outcome ~ x * z + w + \n      (1 | id)\n  )\nsummary(misspecified_lmer)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: outcome ~ x * z + w + (1 | id)\n   Data: nonlinear_dat\n\nREML criterion at convergence: 5730\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.7929 -0.6616 -0.1518  0.4770  4.5718 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept)  0.7794  0.8828  \n Residual             12.0680  3.4739  \nNumber of obs: 1065, groups:  id, 71\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   6.7859     0.1498  45.312\nx             1.3789     0.1153  11.961\nz             2.0853     0.1093  19.087\nw             0.1155     0.1093   1.057\nx:z           0.1555     0.1160   1.341\n\nCorrelation of Fixed Effects:\n    (Intr) x      z      w     \nx   -0.069                     \nz    0.016 -0.011              \nw    0.016 -0.023 -0.031       \nx:z -0.006 -0.049 -0.140 -0.004\n\nIf you had a highly experienced theorist or a whiz analyst, they perhaps could figure out how to correctly specify an MLM, but it would not be quick. This is a three-way interaction, and one where any two-way interaction would either eat away statistical power (or even result in a type I error). that would be specified with this formula: outcome ~ x + z + x * z * dummy_code + w +  (1 | id), which estimates 4 unnecessary effects. In a smaller sample, potential issues with this are amplified\nBelow is the correctly specified model, which technically provides statistical test that the slope of each dummy coded group, formed with the condition x * z < 0, is different from 0. This would be a very difficult model to simply happen upon.\n\n\nshow code\n\nnonlinear_dat <- \n  nonlinear_dat %>% \n  mutate(\n    dummy_code = \n      factor(if_else(x*z < 0, 0, 1))\n      )\n\nconfirmation_lmer2 <- \n  lmer(\n    data = nonlinear_dat, \n    formula = \n      outcome ~ x + z + x:z:dummy_code + w +  \n      (1 | id)\n  )\nsummary(confirmation_lmer2)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: outcome ~ x + z + x:z:dummy_code + w + (1 | id)\n   Data: nonlinear_dat\n\nREML criterion at convergence: 4727.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5431 -0.6796 -0.0225  0.6703  2.9410 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept) 0.7424   0.8616  \n Residual             4.5122   2.1242  \nNumber of obs: 1065, groups:  id, 71\n\nFixed effects:\n                Estimate Std. Error t value\n(Intercept)      4.48729    0.13363  33.579\nx                1.11054    0.07152  15.528\nz                1.96038    0.06733  29.117\nw                0.07117    0.06740   1.056\nx:z:dummy_code0 -3.75376    0.11871 -31.621\nx:z:dummy_code1  3.87560    0.11535  33.598\n\nCorrelation of Fixed Effects:\n            (Intr) x      z      w      x:z:_0\nx           -0.011                            \nz            0.029 -0.010                     \nw            0.016 -0.023 -0.027              \nx:z:dmmy_c0  0.331  0.041 -0.049  0.008       \nx:z:dmmy_c1 -0.331 -0.100 -0.122 -0.011 -0.253\n\nExploring the Data with the {glmertree} package\nLet’s see how GLMM trees act in my next post\n\n\n\n",
    "preview": "blog/simulating-2-level-data-for-apa-2021-supplemental/simulating-2-level-data-for-apa-2021-supplemental_files/figure-html5/figure 1asdf-1.png",
    "last_modified": "2021-08-09T10:57:18-07:00",
    "input_file": "simulating-2-level-data-for-apa-2021-supplemental.utf8.md"
  },
  {
    "path": "blog/simulating-multilevel-linear-data/",
    "title": "Simulating Multilevel Linear Data with Interaction",
    "description": "function to simulate multilevel data with interaction.",
    "author": [
      {
        "name": "Christopher Loan",
        "url": {}
      }
    ],
    "date": "2021-07-28",
    "categories": [
      "Multi-Level Modeling",
      "MLM",
      "Plotting",
      "Simulated Data",
      "Tutorial"
    ],
    "contents": "\n\nContents\nDisclaimer\nGoal\nFunction to simulate 2 level data with an interaction\nSimulating data\nDescribing data & verifying simulation results\nDistribution of Variables\nDistribution of Group Averages\n\nFitting multilevel model to the data\n\n\n\nshow code\n\nlibrary(tidyverse)\nlibrary(glmertree)\n\n\n\nDisclaimer\nMy code is available for use and alteration, but please give credit with a citation & link to the page.\nGoal\nShare code for simulating multilevel data, with optional interaction term.\nFunction to simulate 2 level data with an interaction\nThis function allows you to specify sample sizes, two main effects (main_x & main_z), an interaction, and other features of the data. If you specify interaction = 0, you can simply specify multilevel data with no interaction term. Of course the code could be extended to include as many effects as you need. It also provides a variable (w) that’s not associated with the outcome (outcome).\n\n\nshow code\n\nsimulate_interaction_w_2_levels <-\n  function(\n    n, ## number level 1 units\n    j, ## number level 2 units\n    intercept_lv1, ## intercept at level 1\n    main_x, ## main effect of x\n    main_z, ## main effect of z\n    interaction, ## interaction of x and z\n    residual_var_sd_lv1, ## standard deviation of residuals at level 1\n    random_int_mean_lv2, ## mean of random intercept at level 2\n    random_int_sd_lv2, ## standard deviation of random intercept at level 2,\n    start_seed = 123\n  ){\n    \n    ## placeholder for level 2 outcomes\n    outcomes_j <- vector(\"list\", j)\n    \n    ## for each variable, make list \n    ## as long as level 2\n    ## fill each element with a list of level 1 outcomes\n    \n    \n    x <- vector(\"list\", j) \n    z <- vector(\"list\", j) \n    w <- vector(\"list\", j) \n    \n    ## School distribution (intercept at level 2)\n    ## Standard deviation of random intercept at level 2\n    set.seed(start_seed)\n    \n    a_j <- rnorm(j, random_int_mean_lv2, random_int_sd_lv2)\n    \n    for(i in 1:j) {\n      \n      ## make a level 1 predictor variable:\n      ## set multiple seeds that change each iteration\n      ## prevents identical cases with seed\n      \n      set.seed(start_seed+i)\n      x[[i]] <- rnorm(n)\n      set.seed(start_seed-i)\n      z[[i]] <- rnorm(n)\n      set.seed(-start_seed + i)\n      outcomes_j[[i]] <- \n        rnorm(\n          n, \n          intercept_lv1 + \n            interaction*z[[i]]*x[[i]] + \n            main_x*x[[i]] + \n            main_z*z[[i]] + \n            a_j[i], \n          ## standard deviation of residual variance\n          residual_var_sd_lv1\n        )\n      set.seed(start_seed*197+197*i)\n      w[[i]] <- rnorm(n)\n    }\n    \n    outcomes_df <- \n      data.frame(\n        id = rep(1:j, each = n),\n        x = unlist(x),\n        z = unlist(z),\n        outcome = unlist(outcomes_j),\n        w = unlist(w)\n      )\n    \n    return(outcomes_df)\n  }\n\n\n\n\n\nshow code\n\n# baseline theme for plots here\nproject_theme <- \n  theme(\n    axis.line = element_line(size = 3, lineend = 'round'), \n    legend.position = 'none',\n    plot.title.position = 'plot', \n    panel.grid.minor = element_blank(),\n    axis.title.x = element_text(size = 18),\n    axis.text = element_text(size = 16),\n    axis.title.y = element_text(size = 20),\n    plot.title = element_text(size = 25),\n    strip.text = element_text(size = 25),\n  )\n\n\n\nSimulating data\nWe have 15 level 1 units, 71 level 2 units. The level 1 intercept of the outcome is 4.25 with a main effect of 1.25 for x, 2.00 for z, and 0.75 for their interaction. The function also makes a nuisance variable w, which does not contribute to outcome at all.\n\n\nshow code\n\nsim_dat <- \n  simulate_interaction_w_2_levels(\n    n = 15, ## number level 1 units at each level 2\n    j = 71, ## number level 2 units\n    intercept_lv1 = 4.25, ## intercept at level 1\n    main_x = 1.25, ## main effect of x\n    main_z = 2.00, ## main effect of z\n    interaction = 0.75, ## interaction of x and z\n    residual_var_sd_lv1 = 2.00, ## standard deviation of residuals at level 1\n    random_int_mean_lv2 = 0, ## mean of random intercept at level 2\n    random_int_sd_lv2 = 1.00, ## standard deviation of random intercept at level 2,\n    start_seed = 123 ## ensure you can reproduce \n  ) %>% \n  tibble()\n\n\n\nPut formally, the simulation is approximating:\n\\[\n\\begin{aligned}\n  \\operatorname{outcome}_{i}  &\\sim N \\left(\\mu, \\sigma^2 \\right) \\\\\n    \\mu &=\\alpha_{j[i]} + \\beta_{1}(\\operatorname{x}) + \\beta_{2}(\\operatorname{z}) + \\beta_{3}(\\operatorname{w}) + \\beta_{4}(\\operatorname{x} \\times \\operatorname{z}) \\\\\n    \\alpha_{j}  &\\sim N \\left(\\mu_{\\alpha_{j}}, \\sigma^2_{\\alpha_{j}} \\right)\n    \\text{, for id j = 1,} \\dots \\text{,J}\n\\end{aligned}\n\\]\nwhere\n\\[\\mu = 4.25\\] \\[\\sigma = 2.00\\] \\[\\beta_{1} = 1.25\\] \\[\\beta_{2} = 2.00\\] \\[\\beta_{3} = 0\\] \\[\\beta_{4} = 0.75\\] \\[J = 71\\] \\[\\mu_{\\alpha_{j}} = 0\\] \\[\\sigma_{\\alpha_{j}}= 1.00\\]\nDescribing data & verifying simulation results\nHere are the overall averages for the 1065 cases, with 71 level two units.\n\n\nshow code\n\nsim_dat %>% \n  summarize(\n    x_mean = mean(x), \n    x_sd = sd(x),\n    z_mean = mean(z),\n    z_sd = sd(z),\n    outcome_mean = mean(outcome), \n    outcome_sd = sd(outcome), \n    w_mean = mean(w), # w has no relation to outcome\n    w_sd = sd(w) # w has no relation to outcome\n  )\n\n\n# A tibble: 1 x 8\n  x_mean  x_sd  z_mean  z_sd outcome_mean outcome_sd  w_mean  w_sd\n   <dbl> <dbl>   <dbl> <dbl>        <dbl>      <dbl>   <dbl> <dbl>\n1 0.0900 0.946 -0.0197  1.00         4.44       3.38 -0.0208 0.993\n\nDistribution of Variables\n\n\nshow code\n\nfigure_1 <- \n  sim_dat %>% \n  pivot_longer(\n    cols = -id, \n    names_to = 'variable',\n    values_to = 'statistic'\n    ) %>% \n  group_by(variable) %>% \n  mutate(average_value = mean(statistic)) %>% \n  ungroup() %>% \n  ggplot(\n    aes(\n      x = statistic, \n      fill = variable)\n  ) +\n  colorblindr::scale_fill_OkabeIto() +\n  colorblindr::scale_color_OkabeIto() +\n  geom_vline(aes(xintercept = average_value), size = 1.25, linetype = 2) +\n  geom_density(alpha = 0.7, color = 'black', size = 2) +\n  facet_wrap(~variable, scales = 'free') +\n  labs(\n    y = element_blank(),\n    x = 'Distribution of Simulated Values', \n    title = 'Figure 1. Distribution of Simulated Variables', \n    caption = 'Note the scales are free on each plot\\nDotted Lines = Average Values') +\n  theme_minimal() +\n  project_theme + \n  theme(axis.text.y = element_blank())\nfigure_1\n\n\n\n\nWhen you group by the id, the simulated averages are normally distributed around the values (Figure 1).\nDistribution of Group Averages\n\n\nshow code\n\nfigure_2 <- \n  sim_dat %>% \n  group_by(id) %>% \n  summarize(\n    x = mean(x), \n    z = mean(z),\n    outcome = mean(outcome), \n    w = mean(w)\n    ) %>% \n  pivot_longer(\n    cols = -id, \n    names_to = 'variable',\n    values_to = 'statistic'\n    ) %>% \n  group_by(variable) %>% \n  mutate(average_value = mean(statistic)) %>% \n  ungroup() %>% \n  ggplot(\n    aes(\n      x = statistic, \n      fill = variable)\n  ) +\n  colorblindr::scale_fill_OkabeIto() +\n  colorblindr::scale_color_OkabeIto() +\n  geom_vline(aes(xintercept = average_value), size = 1.25, linetype = 2) +\n  geom_density(alpha = 0.7, color = 'black', size = 2) +\n  facet_wrap(~variable, scales = 'free') +\n  labs(\n    y = element_blank(),\n    x = 'Distribution of Simulated Values (group averages)', \n    title = 'Figure 2. Distribution of Group Averages for\\nSimulated Variables', \n    caption = 'Note the scales are free on each plot') +\n  theme_minimal() +\n  project_theme + \n  theme(axis.text.y = element_blank())\nfigure_2\n\n\n\n\n\n\nshow code\n\nfigure_3a <-   \n  sim_dat %>% \n  ggplot(aes(x = x, y = outcome)) + \n  geom_point() +\n  geom_smooth(method = 'lm') +\n  theme_minimal() +\n  project_theme +\n  labs(\n    title = 'Figure 3. Association of Outcome & Each Covariate',\n    subtitle = '3a. Association of Outcome & x'\n    ) +\n  theme(plot.subtitle = element_text(size = 15))\n\nfigure_3b <-   \n  sim_dat %>%\n  ggplot(aes(x = z, y = outcome)) + \n  geom_point() +\n  geom_smooth(method = 'lm') +\n  theme_minimal() +\n  project_theme +\n  labs(subtitle = '3b. Association of Outcome & z') +\n  theme(plot.subtitle = element_text(size = 15))\n\nfigure_3c <- \n  sim_dat %>% \n  ggplot(aes(x = x*z, y = outcome)) + \n  geom_point() +\n  geom_smooth(method = 'lm') +\n  theme_minimal() +\n  project_theme +\n  labs(subtitle = '3c. Association of Outcome & Interaction') +\n  theme(plot.subtitle = element_text(size = 15))\n\nggpubr::ggarrange(figure_3a, figure_3b, figure_3c, ncol = 1)\n\n\n\n\nFitting multilevel model to the data\nThe simulation looks to have worked as planned. With larger samples, the effects becomes closer to what we specified, obviously, but that was omitted for brevity. Let’s see what this data looks like with an {lme4} to be sure.\n\n\nshow code\n\nconfirmation_lmer <- \n  lmer(\n    data = sim_dat, \n    formula = outcome ~ x * z + w + # equivalent to x + z + x : z\n      (1 | id)\n  )\nsummary(confirmation_lmer)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: outcome ~ x * z + w + (1 | id)\n   Data: sim_dat\n\nREML criterion at convergence: 4729.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4943 -0.6663 -0.0260  0.6799  2.9690 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n id       (Intercept) 0.7374   0.8587  \n Residual             4.5271   2.1277  \nNumber of obs: 1065, groups:  id, 71\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  4.37559    0.12117  36.111\nx            1.09797    0.07135  15.389\nz            1.95442    0.06737  29.011\nw            0.06962    0.06750   1.031\nx:z          0.80677    0.07160  11.268\n\nCorrelation of Fixed Effects:\n    (Intr) x      z      w     \nx   -0.053                     \nz    0.012 -0.014              \nw    0.013 -0.024 -0.027       \nx:z -0.005 -0.049 -0.141 -0.003\n\nWe see how closely the MLM estimates identify the simulated effects. We see small standard errors and large t-values for these estimates, too. As we’d like to see, w does not have significant associations with the outcome.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-06T16:44:16-07:00",
    "input_file": {}
  },
  {
    "path": "blog/disc-golf-web-traffic/",
    "title": "Quantifying Pandemic-Era Growth in Disc Golf by Webtraffic",
    "description": "Trying to figure out why the disc golf course is so crowded these days, with code.",
    "author": [
      {
        "name": "Christopher Loan",
        "url": {}
      }
    ],
    "date": "2020-12-26",
    "categories": [
      "Side Projects",
      "Google Trends",
      "Public Data",
      "Plotting",
      "Geographic Data"
    ],
    "contents": "\n\nContents\nAverage Webtraffic by Year\nFigure 1.\nFigure 2.\n\nVisualizing Monthly Trends over Time\nFigure 3.\nFigure 4.\nTable 1.\nFigure 5.\n\nWhere (at least within the USA) is the Webtraffic Located?\nFigures 6.\n\nMaps (Figures 7-10.) Search Popularity by State in 2017-2020\n\nAnyone who plays disc golf has heard that disc golf grew this year—it’s been a very easy socially distant activity, and it’s awesome. I actually coached a disc golf team in 2015-2017 while I was a high school biology teacher, so it’s near and dear to me. A large part of me is happy for the growth, but I’d be lying if I didn’t admit a small part of me is annoyed how busy the courses are now, hah! While waiting on a teepad, I had an idea to quantify this and I hadn’t seen anyone try to quantify it yet.\nI’m going to generally refer to the growth of disc golf based on it’s search popularity in Google Trends. This is an imperfect proxy for the overall growth of disc golf, but I am okay with that, and it’s my only source of data for this project. \nEvery time someone is looking for a disc, a nearby course, a YouTube tutorial, or a bit of Disc Golf Pro Tour coverage, they probably search for this on Google (or something owned by Google, i.e., YouTube). \nHere’s what I hope to answer:\nHas disc golf grown since 2004?\nHow much of that growth was witnessed in 2020?\nWhere in the country (United States) is all the webtraffic occuring?\n\n\nshow code\n\nknitr::include_graphics(here::here(\"images\", \"ted-johnson.jpeg\"))\n\n\n\n\nphoto by Ted Johnson\nBefore we hop in, pay attention to the units of relative popularity that Google gives:\n“Numbers represent search interest relative to the highest point on the chart for the given region and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. A score of 0 means there was not enough data for this term.”\nSo basically, when I query data from 2004-2020, all data will be scaled with 100 being the peak popularity at any time in that window. I have no information on absolute numbers, just change. Anyways, let’s dive in.\nThis GitHub Repo has all my data if you’re interested.\n\n\nshow code\n\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(here)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(knitr)\nlibrary(usmap)\nlibrary(glmertree)\nlibrary(ggthemes)\ntheme_set(theme_economist())\n\n\n\n\n\nshow code\n\ndat <- import(\"geoMap 2017.csv\") %>% \n  left_join(import(\"geoMap 2018.csv\")) %>% \n  left_join(import(\"geoMap 2019.csv\")) %>% \n  left_join(import(\"geoMap 2020.csv\")) %>% \n  mutate(Region = factor(Region)) %>% \n  rename(`2017` = 'disc golf: (2017)',\n         `2018` = 'disc golf: (2018)',\n         `2019` = 'disc golf: (2019)',\n         `2020` = 'disc golf: (2020)')\n\ndat_trend <- \n  import(\"Search DG Since 2004.csv\") %>% \n  transmute(Date = \n              ymd(parse_date_time(date, \"ym\")),\n            searches = searches, \n            year = year(Date), \n            month = month(Date))\n\n\n\nAverage Webtraffic by Year\nLet’s start with average Google Searches of with words “Disc Golf” over time every year. I have data for each month, and we can show the variation across months in these error bars. Where they do not overlap, we have significant differences.\nFigure 1.\n\n\nshow code\n\nmonths_vector <- \n  c('Jan', 'Feb', 'Mar', 'April', 'May', 'Jun',\n    'July', 'Aug', 'Sept', 'Oct', 'Nov', 'Dec')\n\ndat_trend %>% \n  group_by(year) %>% \n  summarize(mean_yearly = mean(searches),\n            sd = sd(searches), \n            se_yearly = sd/sqrt(n())) %>% \n ggplot(aes(x = year, \n            y = mean_yearly, \n            ymin = mean_yearly-1.96*se_yearly, \n            ymax=mean_yearly+1.96*se_yearly\n            ),\n        show.legend = F) + \n  scale_x_continuous(breaks = 2004:2020) +\n  geom_col(fill = '#cc0000', \n           show.legend = F) + \n  geom_point() +\n  geom_errorbar() +\n  geom_line(aes(x = year, y = mean_yearly), show.legend = F) +\n  labs(\n    title = 'Figure 1. Average Yearly Google Search \nPopularity of the Term `Disc Golf`',\n       caption = 'Error Bars Represent 95% Confidence Intervals',\n       y = 'Relative Search Popularity',\n       x = 'Year') + \n  theme(axis.text.x = \n          element_text(angle = 30, vjust = 0.5, hjust=0.5))\n\n\n\n\nAs you can see, 2020 was the first year with a significant change in relative search popularity from the year prior since 2004. Again, keep in mind this is using Google’s scaled units.\nFigure 2.\n\n\nshow code\n\ndat_trend %>% \n  group_by(month) %>% \n  summarize(mean_monthly = mean(searches),\n            sd = sd(searches), \n            se_monthly = sd/n()) %>% \n ggplot(aes(x = month, \n            y = mean_monthly, \n            ymin = mean_monthly - 1.96*se_monthly, \n            ymax = mean_monthly + 1.96*se_monthly,\n            ),\n        show.legend = F) + \n  scale_x_continuous(breaks = 1:12,\n                     labels = months_vector) +\n  geom_col(fill = '#cc0000', \n           show.legend = F) + \n  geom_point() +\n  geom_errorbar() +\n  labs(title = 'Figure 2. Average Monthly Google Search \nPopularity of the Term `Disc Golf`',\n       caption = 'Error Bars Represent 95% Confidence Intervals',\n       y = 'Relative Search Popularity',\n       x = 'Month') + \n  theme(axis.text.x = \n          element_text(angle = 30, vjust = 0.5, hjust=0.5))\n\n\n\n\nIt shouldn’t surprise me, but it really surprises me how clean the distribution of popularity over months are. Consistently, thhe highest searches are in the warmer months, and the colder months get less.\nVisualizing Monthly Trends over Time\nFigure 3.\nAnother way to look at this would be line graphs over time, with different lines for every year. I’ve done that below (Figure 3), and I added a dashed black line for the overall average search popularity for all other years (2004-2019) and then a solid pink (2020 only) line to show search popularity increase this year.\n\n\nshow code\n\ndat_trend %>%\n  ggplot(aes(x = month, \n             y = searches, \n             group = year, \n             color = year)) +\n  scale_x_continuous(breaks = 1:12, \n                     labels = months_vector) +\n  geom_line() +\n  labs(title = 'Figure 3. Relative Search Popularity of \nDisc Golf Every Month Since 2004',\n       y = 'Relative Search Popularity', \n       x = 'Month', \n       color = 'Year') + \n  theme(axis.text.x = \n          element_text(angle = 30, vjust = 0.5, hjust = 1), \n        legend.position = 'right',\n        legend.direction = 'vertical') + \n  geom_hline(\n    aes(yintercept = 45.67188), \n    linetype = 2) + \n  geom_hline(\n    aes(yintercept = 71.41667), \n    color = '#cc0000')\n\n\n\n\nFigure 4.\nFigure 4 shows similar data, except the dashed (orange) line represents all years’ (2004-2020) averages, and the solid (color-coded) lines tell each year’s mean.\n\n\nshow code\n\ndat_plot <- dat_trend %>% \n  group_by(year) %>% \n  mutate(mean_yearly = mean(searches), \n            sd = sd(searches), \n            se_yearly = sd/sqrt(n())) %>% \n  ungroup()\n\ndat_plot %>% \n  ggplot(aes(x = month, y = searches)) + \n  geom_col(aes(fill = year), \n           show.legend = F) + \n  geom_hline(aes(yintercept = mean(searches)), \n             color = 'orange', \n             linetype = 2, \n             show.legend = F) +\n  facet_wrap(~year, ncol = 6) + \n  geom_hline(\n    aes(yintercept = \n          mean_yearly, \n        color = year), \n    show.legend = F) +\n  scale_x_continuous(breaks = 1:12, \n                     labels = months_vector) +\n  labs(\n  title = 'Figure 4. Relative Search Popularity \nof Disc Golf Every Year Since 2004',\ny = 'Relative Search Popularity', \nx = 'Month') + \n  coord_flip() + \n  theme_economist(horizontal = F) +\n  theme(axis.text.y = element_text(size = 7, angle = 45), \n        axis.text.x = element_text(size = 7, angle = 45, vjust = 0.75) \n        )\n\n\n\n\nAnyway you cut it up, disc golf became more popular in terms of Google Search Popularity. Here are the actual numbers.\nTable 1.\nWebtraffic over time\n\n\nshow code\n\ndat_trend %>% \n  group_by(year) %>% \n  rename(Year = year) %>% \n  summarize(`Average Webtraffic` = mean(searches),\n            `Standard Deviation` = sd(searches), \n            `Standard Error` = `Standard Deviation`/sqrt(n())) %>% \n  kable()\n\n\nYear\nAverage Webtraffic\nStandard Deviation\nStandard Error\n2004\n39.66667\n13.91751\n4.017638\n2005\n39.91667\n13.93790\n4.023526\n2006\n39.83333\n14.97169\n4.321955\n2007\n43.41667\n14.07421\n4.062874\n2008\n39.66667\n13.64707\n3.939569\n2009\n44.08333\n13.89871\n4.012213\n2010\n42.25000\n11.20978\n3.235984\n2011\n48.66667\n14.18279\n4.094219\n2012\n50.66667\n16.40030\n4.734358\n2013\n48.25000\n15.87522\n4.582782\n2014\n48.08333\n15.13250\n4.368375\n2015\n50.25000\n15.53369\n4.484189\n2016\n50.16667\n14.91694\n4.306150\n2017\n51.08333\n14.29850\n4.127620\n2018\n46.66667\n12.54326\n3.620927\n2019\n48.08333\n12.71691\n3.671054\n2020\n71.41667\n21.80683\n6.295090\n\nSo how many times (and when) has disc golf trends significantly increased? I’ll spare you the details, but I can do some exploratory analysis with something called a ‘generalized linear mixed effects regression tree’ which is an emerging exploratory technique to find group differences. \nI wrote the model to account for seasonal trends with a random intercept of month, and then I ask the model to tell me between which years differences occured. \nFigure 5.\n\n\nshow code\n\ntree1 <- lmertree(searches ~ 1 | (1 | month) | year, \n         data = dat_trend, \n         cluster = month)\nplot(tree1, ask = F, which = 'tree')\n\n\n\n\nIt looks like the first bit of growth was relatively small, but was significant. This was at year 2011. The window from 2004-2010 had an average of 41.26 of webtraffic, and we saw a significant (but modest) increase (of ~8%) to 49.10 in webtraffic from 2011-2019. These two windows together were significantly different than 2020, which had an average of 71.42! That’s an increase of over 20% from the prior window (2011-2019) \nThis shows no matter how the computer groups the years, there are only 2 significant increases in disc golf search popularity: before 2011 and after 2019. And the latter jump was much larger.\nWhere (at least within the USA) is the Webtraffic Located?\n\n\nshow code\n\ndat_long <- dat %>% \n  pivot_longer(`2017`:`2020`,\n               names_to = \"year\",\n               values_to = \"searches\") %>% \n   mutate(year = factor(year))\n\n\n\nYou can’t get webtraffic trends by state, but you can go in each year and get a single (averaged) snapshot about the relative disc golf webtraffic for a year. So I gathered webtraffic data for 2017-2020 individually and merged the data files. You really need to keep in mind what Google says about this webtraffic for Regions before you look at the data: \n“A higher value means a higher proportion of all queries, not a higher absolute query count. So a tiny country where 80% of the queries are for”bananas\" will get twice the score of a giant country where only 40% of the queries are for “bananas”\nThe vertical line is the average across all months of 2017-2020, and the error bars represent the 95% confidence intervals. It’s pretty clear that Maine is holding it down for Disc Golf Webtraffic (per volume webtraffic), whatever is going on there. Other places (e.g., California) may appear really low there potentially because of a really established disc golf scene which means everyone knows where the courses are / there are in-person pro shops, etc. It also can be conflated with overall webtraffic, so this isn’t as clean as an analysis as above, but it’s still interesting to see who is conducting relatively more searches.\nFigures 6.\n\n\nshow code\n\ndat_long %>% \n  group_by(Region) %>% \n  summarize(mean = mean(searches), \n            sd = sd(searches), \n            se = sd/sqrt(n())#, \n            #popularity = mean*num_courses\n            ) %>% \n  ggplot(aes(y = reorder(Region, mean),\n             x = mean,\n             )) + \n  geom_errorbar(\n    aes(xmin = mean-1.96*se, \n        xmax = mean + 1.96*se, \n        color = mean), \n    show.legend = F) +\n  geom_point(\n    aes(color = mean), \n    show.legend = F) + \n  geom_vline(\n    aes(xintercept = mean(mean))) +\n  labs(title = 'Figure 6. Relative Search \nInterest in `Disc Golf` by State', \n       caption = 'Error Bars Represent 95% Confidence Interval', \n       y = 'State',\n       x = 'Average Relative Search Interest on Google (2017-2020)') + \n  theme_economist(horizontal = F) +\n  theme(axis.text.y = \n          element_text(angle = 30, vjust = 0.5, hjust = 1, size = 5)) \n\n\n\n\n\n\nshow code\n\ndat_long <- dat_long %>% \n  mutate(fips = fips(Region))\ndat_plot2017 <- dat_long %>% \n  filter(year == '2017') \ndat_plot2018 <- dat_long %>% \n  filter(year == '2018')\ndat_plot2019 <- dat_long %>% \n  filter(year == '2019')\ndat_plot2020 <- dat_long %>% \n  filter(year == '2020')\n\n\n\nMaps (Figures 7-10.) Search Popularity by State in 2017-2020\nHere’s the 2017-2020 relative websearch popularity for you visual learners\n\n\nshow code\n\nplot_usmap(data = dat_plot2017,\n           values = 'searches',\n           labels = T, \n           label_color = \"black\",\n           ) + labs(title = 'Figure 7. Search Popularity by State in 2017', \n                    fill = 'Relative Search Popularity')\n\n\n\nshow code\n\nplot_usmap(data = dat_plot2018,\n           values = 'searches',\n           labels = T, \n           label_color = \"black\",\n           )  + labs(title = 'Figure 8. Search Popularity by State in 2018', \n                    fill = 'Relative Search Popularity')\n\n\n\nshow code\n\nplot_usmap(data = dat_plot2019,\n           values = 'searches',\n           labels = T, \n           label_color = \"black\",\n           ) + labs(title = 'Figure 9. Search Popularity by State in 2019', \n                    fill = 'Relative Search Popularity')\n\n\n\nshow code\n\nplot_usmap(data = dat_plot2020,\n           values = 'searches',\n           labels = T, \n           label_color = \"black\",\n           ) + labs(title = 'Figure 10. Search Popularity by State in 2020', \n                    fill = 'Relative Search Popularity')\n\n\n\n\n\n\n\n",
    "preview": "blog/disc-golf-web-traffic/Popularity-of-DG_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-08-03T14:29:51-07:00",
    "input_file": {}
  },
  {
    "path": "blog/webtraffic-analysis-for-otb-discs/",
    "title": "Impact of GKPro Sponsorship on OTB & GKPro Webtraffic ",
    "description": "An example of leveraging free data to benefit a company, with code.",
    "author": [
      {
        "name": "Christopher Loan",
        "url": {}
      }
    ],
    "date": "2020-12-18",
    "categories": [
      "Side Projects",
      "Google Trends",
      "Public Data",
      "SEM",
      "Structural Equation Modeling",
      "Mediation",
      "Bootstrapping",
      "Plotting"
    ],
    "contents": "\n\nContents\nBackground\nMethod\nData Sources:\nGoogle Trends:\nGKPro’s YouTube:\n\nAnalytic Plan:\n\nResults & Discussion\nSignificant Mean Differences (Step 1)\nTrend over Time (Steps 2 & 3)\nInferential Statistics Results (Step 4)\nQuantifying the Increase: Is it significant?\nImpact of number of GK Pro Searches on OTB Searches\nImpact of OTB Sponsored Skins Match on GK Pro Searches\nThe impact of Skins Match Sponsorship on OTB Searches through GK Pro Searches\nTotal Impact of Sponsorship on OTB webtraffic (i.e., overall association)\n\n\nAppendix for Those Interested in Analytic Details:\n\nBackground\nWith COVID-19 decreasing the amount of disc golf content in 2020, I found myself really watching a lot of “skins” matches hosted by GK Productions. Here’s a link to one of the last this season. \nFor those unfamiliar, these matches are more-or-less single-round match-play events. In this format, there is not an overall prize at the end; rather, players compete for cash prizes for taking the best (i.e., lowest) score on each hole. For example, if 3 of 4 players take par on hole 1, and only 1 player gets a birdie, that player gets $50. If 2 (or more) players tie the best score on a given hole, the cash “rolls over” to increase the value of the next hole. Typically, these are filmed around the time (and at the location of) a larger tournament or Disc Golf Pro Tour Event. They feature top pros showing their human side and are a blast to watch. \n“Only The Best” Discs aka OTB Discs has sponsored these events all year, and they’ve offered free shipping on their products–on top of providing cash for the competitors–as a sponsorship promotion. I personally found myself fighting the urge to purchase a disc from them any time I watched a skins match (and often failed the urge). For that reason, I wanted to dive into the data and see what impact these had on OTB webtraffic. As a note, I would prefer to follow this up with sales data before being confident in any impact to their bottom line, but that’s not publicly available. We should note that someone may look at OTB’s website without purchasing anything.\n\n\nshow code\n\nknitr::include_graphics(here::here(\"images\", \"discs.jpeg\"))\n\n\n\n\nMethod\nData Sources:\nGoogle Trends:\nThis gave me search popularity for both Google and YouTube.\nI compared Google’s traffic for “OTB Discs” and “Only the best discs” (I merged the numbers for these) to YouTube’s traffic for “GK Productions”.\nHere’s what Google says this about the values they give on webtraffic: “Numbers represent search interest relative to the highest point on the chart for the given region and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. A score of 0 means there was not enough data for this term.”\nGKPro’s YouTube:\nWhile webtraffic tells me how much GK pro is searched in a week, it doesn’t tell me if a sponsored skins match was released this week or not.\nI manually checked all of the weeks and made a dichotomous (i.e., “Yes” or “No”) variable to let me compare webtraffic for OTB on weeks with and without a skins match.\nAnalytic Plan:\nVisually assess average web traffic for OTB on weeks with and without coverage (Figure 1 below).\nPlot web traffic over time for googling OTB’s website alongside people searching YouTube for GKPro (Figure 2 below).\nPlot the same graph as 2, but adding in trends for searching YouTube and Google for “Disc Golf” generally. This helps us contextualize these rises and falls in OTB webtraffic with seasonal trends. We may expect more googling of OTB during the summer when more people play regardless of the videos (Figure 3 below).\nConduct a formal statistical analysis to see if the effect of a sponsored video significantly increased sales through GKPro videos, controlling for the direct effect of the sponsorship on OTB’s website and seasonal fluxuations in searches of YouTube and Google for “Disc Golf”. This goes one step farther than seeing if there is an association. It’s tough to explain without statistical terms, but this is called “mediation” and it checks if the variation in the link from \\(Sponsorship --> GKPro_{webtraffic}\\) is the same as the variation in \\(GKPro_{webtraffic} --> OTB_{webtraffic}\\). In total this provides a formal test of if the entire path exists, i.e.: \\(Sponsorship --> GKPro_{webtraffic} --> OTB_{webtraffic}\\). This value could be considered a minimum estimate of impact on webtraffic for sponsoring those videos.\nTo be clear, point #4 will be done controlling for the variation of webtraffic for “Disc Golf” generally in both YouTube and Google as a proxy for general disc golf popularity (although I know that’s an imperfect proxy).\nResults & Discussion\nSignificant Mean Differences (Step 1)\n\n\nshow code\n\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(here)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(lavaan)\nlibrary(knitr)\nlibrary(ggthemes)\ntheme_set(theme_economist())\n\n\n\nIf you’re interested, data can be found on at this GitHub Repo\n\n\nshow code\n\ndat <- \n  import(\"OTB_and_GKPro.csv\") %>%\n  clean_names() %>% \n  mutate(week = mdy(week),\n         sponsored = \n           factor(sponsored_skins_video, labels = c('No','Yes')), \n         total_otb = \n           otb_discs_united_states + only_the_best_discs_united_states,\n         Google_DG = disc_golf_united_states, \n         YouTube_DG = disc_golf_youtube_united_states)\ndat_small <- \n  dat %>% \n  select(week, \n         sponsored, \n         total_otb, \n         gk_pro_united_states, \n         Google_DG, \n         YouTube_DG)\n\nplot_dat <- dat_small %>% \n  pivot_longer(cols = total_otb:YouTube_DG, \n               names_to = \"term\", \n               values_to = \"searches\") %>% \n  \n  mutate(`Search Term` = \n           factor(term, \n                  labels = \n                    c( \"GKPro\", \n                       \"Google DG\", \n                       \"OTB\", \n                       \"YouTube DG\")))\n\n\n\nFigure 1 shows that OTB webtraffic is higher in weeks with a skins match (average webtraffic = 57.38), compared to weeks without a skins mathc (average webtraffic = 20.24). Because the standard error bars do not overlap, we can be confident that these are significantly different amounts of webtraffic\n\n\nshow code\n\nplot_dat %>%\n  filter(term == \"total_otb\") %>% \n  group_by(sponsored) %>% \n  summarize(`Average Searches` = mean(searches), \n            `Standard Error of Searches` = sd(searches)/sqrt(n())) %>% \n  ungroup() %>% \n  ggplot(aes(x = sponsored, y = `Average Searches`)) +\n  geom_col(aes(fill = sponsored)) + \n  geom_errorbar(\n    aes(ymin =`Average Searches`-1.96*`Standard Error of Searches`, \n        ymax =`Average Searches`+1.96*`Standard Error of Searches`), \n    width=.2,\n    position = position_dodge(.9)) +\n  labs(title = \"Figure 1. Average Web Traffic on Weeks \nwith and without GK Pro Skins Matches\",\n       caption = \"Error Bars Show 95% Confidence Intervals\",\n       y = \"OTB Web Traffic*\",\n       x = \"Skins Match Same Week\",\n       fill = \"Skins Match\\nSame Week\") \n\n\n\n\n*Note: From Google Trends: “Numbers represent search interest relative to the highest point on the chart for the given region and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. A score of 0 means there was not enough data for this term.”\nTrend over Time (Steps 2 & 3)\nSince we have this nice time-ordered data, I plotted these trends over time. Figure 2 shows Google Searches for OTB Discs alongside YouTube Searches for GK Pro. I highlighted the weeks with sponsored skins matches. The closely coupled peaks and valleys between these lines suggest people may be Googling OTB more often during weeks with a GKPro sponsored skins match; just what we expected!\nSomething important to note here is that OTB actually sponsors other videos, too, so we’d expect some noise here from other videos that are not skins matches (i.e., other GK Pro videos that aren’t skins matches, videos from their sponsored players, etc.). A better model would include those sources of variation, but I didn’t want to scour the internet for all their sponsored players’ videos.\nWe also see an early peak in the model that is before any sponsorsed skins match. A guess I have here is simply that this is the start of the professional disc golf season, and that watching professionial disc golf increases sales of discs. These early spikes that occur for both GK Pro and OTB are likely from the Las Vegas Challenge and the Memorial Championship, which both occured in those times.\n\n\nshow code\n\nplot_dat %>% \n  filter(`Search Term` %in% c('OTB', 'GKPro')) %>% \n  mutate(sponsored_continuous = if_else(sponsored == \"Yes\", 100, 0)) %>% \n  ggplot() + \n  geom_line(\n    aes(x = week, \n        y = searches, \n        color = `Search Term`)) +\n  geom_col(\n    aes(x = week, \n        y = sponsored_continuous), \n    position = \"dodge\", \n    fill = \"hotpink\", \n    alpha = 0.15)+\n  labs(\n  title = \"Figure 2. OTB Discs Google Searches & \nGK Pro YouTube Searches Over Time\",\ncaption = \"Weeks of Sponsored Videos Highlighted with Vertical Bars\",\ny = \"Web Traffic*\", \nx = \"Date\") +\n  scale_color_manual(values = \n                       c(\"cornflowerblue\",\n                         \"coral3\", \n                         \"gray30\", \n                         \"tan\"))+\n  ylim(0,100)\n\n\n\n\nFigure 3 shows the same webtraffic plot, but with the increases in Google and YouTube searches for “Disc Golf” to account for seasonal changes in trends. The early spikes which weren’t associated with sponsorship of a GK Pro match really seem to line up with seasonal trends. But later in the season, it looks like OTB webtraffic went way up compared to the small bump they got from a general increase from the season’s start.\n\n\nshow code\n\nplot_dat %>%\n  ggplot() +\n  geom_line(aes(x = week, y = searches, color = `Search Term`)) +\n  labs(title = \"Figure 3. Webtraffic for OTB & GKPro, \nalongside general `Disc Golf` Searches\",\n       y = \"Web Traffic*\",\n       x = \"Date\") +\n  scale_color_manual(values = \n                       c(\"cornflowerblue\", \n                         \"gray30\", \n                         \"coral3\", \n                         \"tan\")) +\n  ylim(0,100)\n\n\n\n\n*Note: From Google Trends: “Numbers represent search interest relative to the highest point on the chart for the given region and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. A score of 0 means there was not enough data for this term.”\nI used a package called {lavaan} to do some inferential statistics to make sure these trends were significant. I was worried about having so few data points, so I actually did something called “bootstrapping” where I made a statistical model and then simulated the results 5000 times. The 5000 estimates are then used to give some confidence in your findings. This is a huge oversimplification, but I wanted to spare you the details.\nInferential Statistics Results (Step 4)\nWith 95% confidence, we can say that GK Pro videos increase webtraffic for OTB, even controlling for seasonal variation in disc golf popularity. Statistically, we can divide up the influence of these sponsored videos on OTB web traffic into:\nvariability that is “carried through” the GK Pro videos’ variability, and\nunique influence that is not carried through it.\nYou can think of the unique variation as “random noise” that we control for, and the “carried through” variability as the effect of these sponsored videos through GK Pro webtraffic to OTB’s webtraffic. There are explanations for the direct path that are not random noise, but if we’re interested in the mediated effect, this is noise to us. If the effect is in fact carried through GK Pro, we’d expect this to not be significant. \nHowever, one possibility I see is if YouTube recommended a GK Pro video, they won’t actually type it on YouTube, so the direct increase on OTB would look to occur without increased searching of “GK Pro”. I’ll ignore this unless this path is significant, but I don’t expect that \nQuantifying the Increase: Is it significant?\nImpact of number of GK Pro Searches on OTB Searches\nThe model shows that an increase in GK Pro viewership by 1% of their relative webtraffic corresponds to an increase of about 0.43% (95% confidence range = 0.14-0.70). This change is significant and is not small. If GK pro webtraffic went up by 50%, we would see about 21.5% increase on OTB webtraffic. I don’t know the scale of either of their webtraffic, but this seems like a substantial increase. Visually, this is \\(GKPro_{webtraffic} --> OTB_{webtraffic}\\).\nImpact of OTB Sponsored Skins Match on GK Pro Searches\nGK Pro is also (probably) feeling this benefit: if we compare the weeks with sponsored videos to weeks without, their webtraffic is up 28% (95% confidence range = 14.02-42.93) on weeks with sponsored videos compared to weeks without sponsored videos! This is visually \\(Sponsorship --> GKPro_{webtraffic}\\) . The researcher in me knows that this value is conflated with lots of spurrious associations we cannot assess, however, this estimate is controlling for the popularity of “Disc Golf” searches on both YouTube and Google during those weeks. With GK Pro being sponsored most weeks, though, it seems like their popularity may just rise and fall with release of a video (duh!), but that relies on someone sponsoring it (and it happens to be OTB). \nThe impact of Skins Match Sponsorship on OTB Searches through GK Pro Searches\nThis is what we are most interested in; it is the minimum impact that is actually felt by OTB through GK Pro. Only variation which is consistent through both variables is accounted for here. Visually, it looks like:\n\\[Sponsorship --> GKPro_{webtraffic} --> OTB_{webtraffic}\\] This is the “mediated” effect we’re interested in. In essence, OTB is boosting their own webtraffic by 12.20% (95% confidence range = 2.64-26.6%) for every video they sponsor. They can use this to determine how “worth it” these videos are to them. \nTotal Impact of Sponsorship on OTB webtraffic (i.e., overall association)\nIt is worth noting that when we control for YouTube and Google searches for “Disc Golf” and the searches for GK Pro, we see no direct effect of sponsorship on OTB searches. This likely means that most people searching OTB are doing so because of GK Pro.\nAppendix for Those Interested in Analytic Details:\nI included minimal output from the regression below, and you can see it’s significant if the ci.lower and ci.upper have the same term (i.e., - or +), which represent a range of confidence in our findings at 95%. The defined parameter “mediated_effect” is the product of the paths a*b, which is the most common way to formally test mediation in structural equation modeling.\n\n\nshow code\n\nmediation_mod1 <- '\ntotal_otb ~ a*gk_pro_united_states + d*sponsored + Google_DG + YouTube_DG\n\ngk_pro_united_states ~ b*sponsored+ Google_DG + YouTube_DG\n\nmediated_effect := a*b\n'\n\nmed_fit_boot<- sem(mediation_mod1, \n                   data = dat, \n                   estimator = \"ML\",\n                   se = \"bootstrap\", \n                   bootstrap = 5000,\n                   parallel = c(\"multicore\"))\n\ntable <- parameterEstimates(med_fit_boot, \n                   level = .95, \n                   boot.ci.type = \"perc\", \n                   output = \"pretty\", \n                   stand = T) \n\ntable %>% \n  select(-c(se:pvalue), \n         -c(std.lv:std.nox)\n         ) %>% \n  filter(op!='~~')\n\n\n\nRegressions:\n                         Estimate ci.lower ci.upper\n  total_otb ~                                      \n    gk_pr_ntd_ (a)          0.431    0.144    0.697\n    sponsored  (d)         14.476   -1.682   29.606\n    Google_DG               0.163   -0.161    0.475\n    YouTube_DG              0.292   -0.053    0.661\n  gk_pro_united_states ~                           \n    sponsored  (b)         28.326   13.681   42.869\n    Google_DG               0.328   -0.067    0.705\n    YouTube_DG             -0.358   -0.753    0.059\n\nDefined Parameters:\n                   Estimate ci.lower ci.upper\n    mediated_effct   12.196    2.757   25.821\n\n\n\n\n",
    "preview": "blog/webtraffic-analysis-for-otb-discs/OTB_GKPro_Sales_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-08-03T14:30:22-07:00",
    "input_file": {}
  },
  {
    "path": "blog/growth-ppm/",
    "title": "Introduction to Growth & Parallel Process Models in `{lavaan}`",
    "description": "A tutorial to parameterize and plot results of a parallel process model (PPM) in `{lavaan}`.",
    "author": [
      {
        "name": "Christopher Loan",
        "url": {}
      }
    ],
    "date": "2020-09-08",
    "categories": [
      "SEM",
      "Structural Equation Modeling",
      "Mediation",
      "Bootstrapping",
      "Plotting",
      "Simulated Data",
      "Tutorial",
      "Model Comparisons"
    ],
    "contents": "\n\nContents\nIntroduction:\nLoad packages\nSimulating Data for Demonstration\nFit statistics\nSteps for a PPM:\nSTEP 1. Plot your data\nSTEP 2. Assess fit of various models\nFunction\nEstimator\nMissingness\n\nSTEP 3. Building the PPM\nFull Model Fit\nInterpretation\nBootstrapping\nPlotting\nNext Steps:\nLooking at the simulation call\n\n\n\nshow code\n\nknitr::include_graphics(here::here(\"images\", \"tomas-sobek.jpeg\"))\n\n\n\n\nphoto by Tomas Sobek\nIntroduction:\nDuring my MS, I had some research questions that were easily answered with structural equation modeling (SEM). I decided to learn {lavaan} because it’s open source and allows simple integration with other R packages. \nBecause I relied heavily on publicly available tutorials, I wanted to create a {lavaan} tutorial based on a method I used several times: Growth and Parallel Process Models (PPMs).\nMost of the tutorials I have found explain only one factor (e.g., the growth curve page on the {lavaan} website) and putting two of these together can be daunting to a new R or {lavaan} user.\nThink of this more as a tutorial of the steps needed to take when fitting growth models in {lavaan}, rather than a tutorial solely on either the method or the software.\nAs a quick terminology aside: I refer to growth models where 2+ factors are modeled “in parallel over time” as parallel process models (PPMs), but they are sometimes referred to differently. \nNOTES:\nBeing a SEM method, this assumes some familiarity with SEM, though I hope to present this in a way that is clear to as many people as possible.\nI expect you to understand or to be able to refer to {lavaan} syntax for basic syntax/interpretation.\nConsider what I give you the nuts and bolts in how to make this work, and you should refer to other resources for more in depth interpretations.\nLoad packages\n\n\nshow code\n\nlibrary(lavaan)\nlibrary(tidyverse)\nlibrary(ggthemes)\ntheme_set(theme_economist())\n\n\n\nSimulating Data for Demonstration\nIn order to complete this project, I simulated some data (n = 500). For simplicity, I actually used {lavaan} to make my data, but other software can do this too.  I am reserving the code for this until the end, in case you would like to know how I did that, or if you’re curious to see how closely our PPM approximates the structure of the data we made.  Specifically, I made two variables across 4 waves (growth modeling assumes time to be equally spaced).  I named the variables {x1} ={x}at time 1, {x2} = {x} at time 2, etc. & {y1} = {y} at time 1, etc.\nFit statistics\nIn SEM, goodness-of-fit testing is not free; you need degrees of freedom. Specifically, you can calculate fit on models where the highest term is {x^(n-2)} where {n} is the number of waves you have. I wanted to show a quadratic fitting process, so I included 4 waves (i.e., x^(4-2) = x^2-order models can be assessed for fit).\nIt’s good practice to have a set of fit statistcs specified a priori so you aren’t cherry picking good and bad results. Here’s a spread of them I’ve selected, along with ideal levels of fit (commented out).\n\n\nshow code\n\nselected_fit_stats <-   \n  c(\"chisq.scaled\",\n    \"df.scaled\", ## must be >0 to test G.O.F.\n    \"pvalue.scaled\", ## ideally n.s.\n    \"cfi.scaled\", ## ideally ≥ 0.95\n    \"rmsea.scaled\", ## ideally ≤ 0.05\n    \"rmsea.pvalue.scaled\", ## ideally n.s.\n    \"srmr\" ## ideally < 0.08\n    )\n\n\n\nSteps for a PPM:\nPlot your data\nAssess fit of various models for both variables (i.e., no growth, linear growth, quadratic growth, etc.).\nCombine individual ideal models\nAssess fit (& modify syntax, typically, but that is nuanced & beyond our scope)\nPlot the predicted data & compare to step 1\nSTEP 1. Plot your data\nThis should give you a guess at what forms probably will fit. I did some work in {tidyr} to move the data around, then plotted in {ggplot2}, where we can see individual trajectories for each construct over time. I set the alpha parameter very low (0.2) so lines are transparent and we can get at density of distribution.\n\n\nshow code\n\nsim_growth_dat$participant_n <- \n  1:nrow(sim_growth_dat) #add participant number\n\nx_plot <-\n  pivot_longer(sim_growth_dat, \n               cols = x1:x4, \n               names_to = 'x',\n               names_prefix = 'x')\n\nindividual_x_trajectories <- \n  ggplot(x_plot, \n         aes(x = as.numeric(x),\n             y = value, \n             group = participant_n, \n             color = participant_n)) +\n  geom_line(alpha = 0.2) +\n  labs(title = 'Observed Trajectories of x', \n       x = 'Timepoint', \n       y = 'x') +\n  xlim(1,4) +\n  theme(legend.position = 'none')\n\nindividual_x_trajectories\n\n\n\n\nYou can see that the {x} variable is probably linearly increasing, but a very “shallow” quadratic model might fit better so we should check if it fits well. If both fit well, we will compare them with likelihood ratio tests (LRTs).\n\n\nshow code\n\ny_plot <-\n  pivot_longer(sim_growth_dat, \n               cols = y1:y4, \n               names_to = 'y',\n               names_prefix = 'y')\n\nindividual_y_trajectories <- \n  ggplot(y_plot, \n         aes(x = as.numeric(y), \n             y = value, \n             group = participant_n, \n             color = participant_n)) + \n  geom_line(alpha = 0.2) +\n  labs(title = 'Observed trajectories of y', \n       x = 'Timepoint', \n       y = 'y') +\n  xlim(1,4) +\n  theme(legend.position = 'none')\n\nindividual_y_trajectories\n\n\n\n\nIt is clear that {y} decreases with time on average, but there is heterogeneity (or variability) in what happens for an individual case. It looks like a either a linear or quadratic model will fit best. \nLet’s dive in!\nSTEP 2. Assess fit of various models\nTime to first fit the model to an intercept-only model (i.e., no growth observed).\nNext, we’ll look at linear growth (linear + intercept), then quadratic growth (quadratic + linear + intercept). We won’t go higher than that because we cannot assess fit with only 4 waves.\nThere are lots of nuances here, but I discuss some common choices here.\nFunction\nThe growth() call in {lavaan} is an easy way to specify a growth model. By default, it sets means/intercepts of observed variables to 0 and determines mean structure for latent variables.\nEstimator\nAll data used here is continuous, so we’ll stick with maximum likelihood (estimator = ML) estimation, and I prefer to use the “robust” variant (estimator = MLR) whenever possible to account for non-normal data. \nMissingness\nFor simplicity, there is no missingness in this data. If there were, we could do Little’s Test of Missing data and see patterns of missingness. Assuming data was missing (completely) at random and we didn’t have too much missingness, we could use Full-information Maximum Likelihood to impute missing data. \n\n\nshow code\n\nint_x_mod <- \n  '\niX =~ 1*x1 + 1*x2 + 1*x3 + 1*x4\n\n  '\nint_x_fit <- \n  growth(model = int_x_mod,\n         estimator = 'MLR',\n         data = sim_growth_dat)\n\nint_x_fit_stats <-\n  fitmeasures(\n    int_x_fit, \n    selected_fit_stats) %>% \n  data.frame()\n\nround(int_x_fit_stats, 2)\n\n\n                          .\nchisq.scaled        1192.66\ndf.scaled              8.00\npvalue.scaled          0.00\ncfi.scaled             0.02\nrmsea.scaled           0.54\nrmsea.pvalue.scaled    0.00\nsrmr                   0.63\n\nThe no growth model for {x} does not fit well by any measure. So we’ll move on.\n\n\nshow code\n\nint_y_mod <- \n  '\niY =~ 1*y1 + 1*y2 + 1*y3 + 1*y4\n  '\nint_y_fit <- \n  growth(model = int_y_mod, \n         estimator = 'MLR',\n         data = sim_growth_dat)\n\nint_y_fit_stats <- \n  fitmeasures(\n    int_y_fit, \n    selected_fit_stats) %>%\n  data.frame()\n\nround(int_y_fit_stats, 2)\n\n\n                          .\nchisq.scaled        3136.54\ndf.scaled              8.00\npvalue.scaled          0.00\ncfi.scaled             0.00\nrmsea.scaled           0.88\nrmsea.pvalue.scaled    0.00\nsrmr                   0.98\n\nThe no growth model for {y} does not fit well by any measure either.  Next, we will check how well a linear-only growth model fits for the {x} & {y} variables\n\n\nshow code\n\nlinear_x_mod <- \n  '\niX =~ 1*x1 + 1*x2 + 1*x3 + 1*x4\nsX =~ 0*x1 + 1*x2 + 2*x3 + 3*x4\n\n  '\nlinear_x_fit <- \n  growth(model = linear_x_mod,\n         estimator = 'MLR',\n         data = sim_growth_dat)\n\nlinear_x_fit_stats <- \n  fitmeasures(linear_x_fit, \n              selected_fit_stats) %>%\n  data.frame()\n\nround(linear_x_fit_stats, 2)\n\n\n                       .\nchisq.scaled        4.03\ndf.scaled           5.00\npvalue.scaled       0.54\ncfi.scaled          1.00\nrmsea.scaled        0.00\nrmsea.pvalue.scaled 0.92\nsrmr                0.02\n\nThe linear growth model for {x} fits well by all measures. This is what we expected based on the graph of observed data. We should test quadratic models for fit, though.\n\n\nshow code\n\nlinear_y_mod <- \n  '\niY =~ 1*y1 + 1*y2 + 1*y3 + 1*y4\nsY =~ 0*y1 + 1*y2 + 2*y3 + 3*y4\n  '\nlinear_y_fit <- \n  growth(model = linear_y_mod,\n         estimator = 'MLR',\n         data = sim_growth_dat)\n\nlinear_y_fit_stats <- \n  fitmeasures(\n    linear_y_fit, \n    selected_fit_stats) %>%\n  data.frame()\n\nround(linear_y_fit_stats, 2)\n\n\n                         .\nchisq.scaled        842.85\ndf.scaled             5.00\npvalue.scaled         0.00\ncfi.scaled            0.61\nrmsea.scaled          0.58\nrmsea.pvalue.scaled   0.00\nsrmr                  0.55\n\nLooks like the linear {y} is inadequate across all measures. \nNOTE:\nQuadratic terms represent the average rate of change of the slope across waves. This is synonymous with acceleration in other disciplines, though this terminology is not often used and ‘quadratic growth’ is typically preferred in SEM.\n\n\nshow code\n\nquad_x_mod <- \n  '\niX =~ 1*x1 + 1*x2 + 1*x3 + 1*x4\nsX =~ 0*x1 + 1*x2 + 2*x3 + 3*x4\nqX =~ 0*x1 + 1*x2 + 4*x3 + 9*x4\n\n\n  '\nquad_x_fit <- \n  growth(model = quad_x_mod, \n         estimator = 'MLR',\n         data = sim_growth_dat)\n\nquad_x_fit_stats <- \n  fitmeasures(quad_x_fit, \n              selected_fit_stats)\n\nround(quad_x_fit_stats, 2)\n\n\n       chisq.scaled           df.scaled       pvalue.scaled \n               0.02                1.00                0.89 \n         cfi.scaled        rmsea.scaled rmsea.pvalue.scaled \n               1.00                0.00                0.94 \n               srmr \n               0.00 \n\nHere we have an interesting issue, as {x} fits fairly well to both a linear & a quadratic model, but we have a Heywood case, specifically the estimated observed variable variance is negative. Let’s inspect \n\n\nshow code\n\nlavInspect(quad_x_fit, \"est\")$theta\n\n\n   x1     x2     x3     x4    \nx1  1.171                     \nx2  0.000  0.718              \nx3  0.000  0.000  1.213       \nx4  0.000  0.000  0.000 -0.064\n\nOur {x4} variable has a negative estimate for the observed variable’s variance. This is a problem, and indicates the model may not represent the data well. There are ways to use parameter constraints to see if the fit is good if we “force” a change to this parameter, but that’s beyond our scope today.\nFor teaching purposes, we’ll pretend we didn’t have a Heywood case to do the next step.\nWhen you have two well-fitting nested models (in our case linear & quadratic models), we can compare the fit formally with Likelihood Ratio Tests (LRTs). The null hypothesis is essentially that there is no difference in the variance explained by the two models. Parsimony is preferred in a situation where the variance explained is equivalent. \nNOTE:\nSome will say this test is only valid if the models have n.s. p-values and there are no Heywood cases.\n\n\nshow code\n\nlavTestLRT(linear_x_fit, quad_x_fit)\n\n\nScaled Chi-Squared Difference Test (method = \"satorra.bentler.2001\")\n\nlavaan NOTE:\n    The \"Chisq\" column contains standard test statistics, not the\n    robust test that should be reported per model. A robust difference\n    test is a function of two standard (not robust) statistics.\n \n             Df    AIC    BIC  Chisq Chisq diff Df diff Pr(>Chisq)\nquad_x_fit    1 7570.4 7625.2 0.0200                              \nlinear_x_fit  5 7566.4 7604.3 4.0675     4.0026       4     0.4056\n\nHere, a non-significant p-value tells us that the linear model is preferred here. \nNow onto the quadratic {y} model.\n\n\nshow code\n\nquad_y_mod <- \n  '\niY =~ 1*y1 + 1*y2 + 1*y3 + 1*y4\nsY =~ 0*y1 + 1*y2 + 2*y3 + 3*y4\nqY =~ 0*y1 + 1*y2 + 4*y3 + 9*y4\n\n\n  '\nquad_y_fit <- \n  growth(\n    model = quad_y_mod, \n    estimator = 'MLR',\n    data = sim_growth_dat)\n\nquad_y_fit_stats <- \n  fitmeasures(\n    quad_y_fit, \n    selected_fit_stats\n    )\n\nquad_y_fit_stats\n\n\n       chisq.scaled           df.scaled       pvalue.scaled \n              0.190               1.000               0.663 \n         cfi.scaled        rmsea.scaled rmsea.pvalue.scaled \n              1.000               0.000               0.813 \n               srmr \n              0.001 \n\nIt’s clear that only the {x} linear model was a good one by all fit statistics. We could actually easily see this earlier in Step 1, but it is good that we checked. \nWhen using real—world data this may or may not be the case; I simulated this data specifically to be clean linear and quadratic models for {x} & {y} respectively. \nSTEP 3. Building the PPM\nNow that we are fairly confident in functional forms, let’s model them together. \nNOTE:\nFor simplicity we are not specifying conditional models which make future growth terms conditional upon earlier growth terms. The distinction between the two is subtle and deserves its own conversation later. For now, just assume growth terms are simply correlated, rather than causally linked.\n\n\nshow code\n\nfull_model <- \n  '\n## intercept & slope growth terms for X\niX =~ 1*x1 + 1*x2 + 1*x3 + 1*x4\nsX =~ 0*x1 + 1*x2 + 2*x3 + 3*x4\n\n## intercept, slope, & quadratic terms for Y\niY =~ 1*y1 + 1*y2 + 1*y3 + 1*y4\nsY =~ 0*y1 + 1*y2 + 2*y3 + 3*y4\nqY =~ 0*y1 + 1*y2 + 4*y3 + 9*y4\n\n## regress growth terms on predictor\nqY + iY + sX + iX ~ predictor\nsY ~ a1*predictor\n\n## regress outcome on growth terms\noutcome ~ iX + sX + iY + b1*sY + qY\n\n## testing indirect effect \n## predictor --> sY --> outcome\n\npredictor_sY_outcome:=a1*b1\n  '\n\nfull_fit <- \n  growth(\n    model = full_model, \n    estimator = 'MLR',\n    data = sim_growth_dat)\n\nfull_fit_stats <- \n  fitmeasures(\n    full_fit, \n    selected_fit_stats)\n\nround(full_fit_stats,2) \n\n\n       chisq.scaled           df.scaled       pvalue.scaled \n              28.23               34.00                0.75 \n         cfi.scaled        rmsea.scaled rmsea.pvalue.scaled \n               1.00                0.00                1.00 \n               srmr \n               0.03 \n\nFull Model Fit\nWe can see that this model fits well across all fit statistics chosen. This model is deemed acceptable and we can now interpret the model results\n\n\nshow code\n\nsummary(full_fit)\n\n\nlavaan 0.6-9 ended normally after 65 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        29\n                                                      \n  Number of observations                           500\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                 28.456      28.233\n  Degrees of freedom                                 34          34\n  P-value (Chi-square)                            0.736       0.746\n  Scaling correction factor                                   1.008\n       Yuan-Bentler correction (Mplus variant)                     \n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  iX =~                                               \n    x1                1.000                           \n    x2                1.000                           \n    x3                1.000                           \n    x4                1.000                           \n  sX =~                                               \n    x1                0.000                           \n    x2                1.000                           \n    x3                2.000                           \n    x4                3.000                           \n  iY =~                                               \n    y1                1.000                           \n    y2                1.000                           \n    y3                1.000                           \n    y4                1.000                           \n  sY =~                                               \n    y1                0.000                           \n    y2                1.000                           \n    y3                2.000                           \n    y4                3.000                           \n  qY =~                                               \n    y1                0.000                           \n    y2                1.000                           \n    y3                4.000                           \n    y4                9.000                           \n\nRegressions:\n                   Estimate  Std.Err  z-value  P(>|z|)\n  qY ~                                                \n    predictor         0.041    0.048    0.850    0.395\n  iY ~                                                \n    predictor         0.034    0.060    0.564    0.573\n  sX ~                                                \n    predictor         0.008    0.051    0.161    0.872\n  iX ~                                                \n    predictor         0.067    0.060    1.119    0.263\n  sY ~                                                \n    predictor (a1)    2.022    0.079   25.545    0.000\n  outcome ~                                           \n    iX                1.994    0.145   13.755    0.000\n    sX               -0.017    0.146   -0.115    0.908\n    iY               -0.028    0.186   -0.149    0.881\n    sY        (b1)    2.973    0.104   28.596    0.000\n    qY               -0.242    0.161   -1.510    0.131\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .x1                0.000                           \n   .x2                0.000                           \n   .x3                0.000                           \n   .x4                0.000                           \n   .y1                0.000                           \n   .y2                0.000                           \n   .y3                0.000                           \n   .y4                0.000                           \n   .outcome           0.000                           \n   .iX                1.976    0.059   33.700    0.000\n   .sX                0.991    0.051   19.350    0.000\n   .iY                0.031    0.061    0.508    0.611\n   .sY               -1.153    0.082  -14.019    0.000\n   .qY               -1.548    0.048  -32.151    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)\n   .x1                1.067    0.099   10.777    0.000\n   .x2                0.885    0.069   12.924    0.000\n   .x3                0.983    0.104    9.492    0.000\n   .x4                1.113    0.197    5.646    0.000\n   .y1                0.961    0.107    8.991    0.000\n   .y2                0.868    0.083   10.470    0.000\n   .y3                0.996    0.179    5.557    0.000\n   .y4                2.467    0.949    2.601    0.009\n   .outcome           0.814    0.927    0.878    0.380\n   .iX                1.043    0.100   10.444    0.000\n   .sX                1.094    0.076   14.393    0.000\n   .iY                1.040    0.108    9.595    0.000\n   .sY                1.131    0.125    9.078    0.000\n   .qY                0.927    0.068   13.684    0.000\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(>|z|)\n    predctr_sY_tcm    6.011    0.159   37.874    0.000\n\nInterpretation\nI am going to trust you understand how to read {lavaan} here, so I’m not going to walk through this line by line. The headlines are {iX} & {sY} significantly predict {outcome}, with the latter predicting it to a much larger degree than the former. \nThis means baseline levels of {x} predict outcome, as does the linear rate of change of {y}, both positively such that higher levels of initial {x} or faster rates of change in {y} lead to higher levels of {outcome}.\nIn our example, I included a time-invariant covariate {predictor}. Since {predictor} significantly predicted {sY} and {sY} predicted {outcome}, we would want to look for mediation. You may trust the estimate given by simply the product of each coefficient and it’s significance test, but it is better to rely on other methods to test indirect effects.\nBootstrapping\nBootstrapping is a resampling method (with replacement) that is appreciated for building confidence intervals around parameter estimates of indirect effects, at least in part, because they do not make distributional assumptions about the indirect effect. This creates a more reliable test than standard significance tests. A quick Google Scholar search can inform you on this topic more deeply than this post, but I used this to test the indirect effects using 5000 simulated data sets.\n\n\nshow code\n\nfinal_fit_boot<- \n  growth(full_fit, \n         data = sim_growth_dat, \n         estimator = \"ML\",\n         meanstructure = T,\n         se = \"bootstrap\",\n         bootstrap = 100, # ~5000 better\n         parallel = \"multicore\")\n\nparameterEstimates(\n  final_fit_boot,\n  level = .95,\n  boot.ci.type = \"bca.simple\",\n  stand = T)[61,c(4,5,9,10)]\n\n\n                  label   est ci.lower ci.upper\n61 predictor_sY_outcome 6.011    5.707    6.355\n\nSince this is a tutorial, I set our model to only run 100 times for speed, but you’d likely want to set bootstrap = 5000 for better practice. \nFurthermore, looking at the 95% confidence intervals, we find the same result as we did regarding the indirect effect as without bootstrapping. These are the results I trust more, especially since we used bca.simple. \nMost likely this is what we wanted to do with this model (at least in my field). Assess if a predictor influences growth in a construct, then see if there is an indirect effect (and quanitify its magnitude).\nNOTE:\nI made it so only the label, unstandardized estimate and confidence intervals showed up with the [61,c(4,5,9,10)] portion of code, but you should not trust the p-values from this parameterEstimates() if you go with anything but the default (i.e., boot.ci.type = norm).  This is because the p-value is built assuming a normal distribution; this eliminates some of the benefits of bootstrapping gained by bca.simple and other methods, but gives you a p-value, which matters a lot to some.\nPlotting\nWe can plot the growth of these traits, as well with {ggplot2}, which helps verify you did this correctly. I won’t go into it here, but these plots can be modified with facet wrapping and other grouping/coloring methods to see how different groups have different slopes, for example. \nFirst we extract factor scores & save this as a data frame, then we can plot. For a linear model, this is pretty simple:\n\n\nshow code\n\nplot_dat <- \n  lavPredict(full_fit) %>% \n  data.frame()\n\nplot_dat$participant_n <- \n  1:nrow(plot_dat)\n\nplot_dat$predictor <- \n  sim_growth_dat$predictor\n\npredicted_x_trajectories <- \n  ggplot(data=plot_dat) +\n  scale_x_continuous(\n    name = \"Timepoint\",\n    limits=c(0, 3),\n    breaks = c(0, 1, 2, 3),\n    labels = c('1', '2', '3', '4')\n    ) +\n  scale_y_continuous(\n    name = \"x\", \n    limits=c(-5,15)\n    )+\n  geom_abline(\n    data = plot_dat, \n    mapping = \n      aes(slope=sX, \n          intercept=iX, \n          color = participant_n),\n    alpha = 0.2) + \n  labs(title = 'Predicted trajectories of x')  +\n  theme(legend.position = 'none')\n\n\n\nWe can compare those to the observed trajectories (top) to the predicted trajectories (bottom) with {gridExtra} to merge our plots.\n\n\nshow code\n\ngridExtra::grid.arrange(\n  individual_x_trajectories, \n  predicted_x_trajectories, \n  ncol = 1)\n\n\n\n\nFor a quadratic model, this is more challenging, but here’s some code to do it:\n\n\nshow code\n\nplot_dat <- \n  lavPredict(full_fit) %>%\n  data.frame()\n\nplot_dat$participant_n <- \n  1:nrow(plot_dat)\n\nplot_dat$predictor <-\n  sim_growth_dat$predictor\n\nqY <- plot_dat$qY\nsY <- plot_dat$sY\niY <- plot_dat$iY\n\ntest <- function(y) {qY*y ^ 2 + sY*y + iY}\n\n\nplot_dat_2 <- \n  data.frame(`T1` = test(0), \n             `T2` = test(1), \n             `T3` = test(2), \n             `T4` = test(3), \n             row.names = 1:nrow(plot_dat))\n\nnames(plot_dat) <- c('1', '2', '3', '4') \n\ntest0 <- test(0)\ntest1 <- test(1)\ntest2 <- test(2)\ntest3 <- test(3)\n\nY_df <- \n  c(test0, \n    test1, \n    test2, \n    test3)\n\ndf_long_wave <- \n  c(rep(1,500), \n    rep(2, 500),\n    rep(3, 500), \n    rep(4, 500)\n    )\n\n\nplot_dat_2 <- data.frame(\n  participant_n = \n      rep(1:500,4), \n  Timepoint = \n    df_long_wave,\n  y = Y_df)\n\npredicted_y_trajectories <- \n  plot_dat_2 %>% \n  ggplot(\n    aes(x = Timepoint, \n        y = y, \n        group = participant_n, \n        color = participant_n)) +\n geom_line(formula = y ~ x + I(x^2),\n           method = \"lm\", \n           se = F, \n           alpha = 0.2) +\n  labs(title ='Predicted trajectories of y') +\n  theme(legend.position = 'none')\n\n\n\nAgain, can compare those to the observed trajectories (top) to the predicted trajectories (bottom).\n\n\nshow code\n\ngridExtra::grid.arrange(\n  individual_y_trajectories, \n  predicted_y_trajectories, \n  ncol = 1)\n\n\n\n\nNext Steps:\nIf you were to be working on this, I would work further to make the model more parsimoneous, as well as seeing if alternate modifications would increase model fit. Ways to do this include, but are not limited to, constraining residuals, assessing the role of the predictor on the observed vs. manifest model, making the growth terms conditional upon earlier terms, etc.\nLooking at the simulation call\nHere is the exact simulation call I did in case anyone is curious. I wanted the data to be interesting, so I made sure to set some things (e.g., variances, latent means/intercepts, relationships between variables, etc.)\n\n\nshow code\n\ngrowth_mod <- \n  '\n\n## intercept & slope growth terms for X\niX =~ 1*x1 + 1*x2 + 1*x3 + 1*x4\nsX =~ 0*x1 + 1*x2 + 2*x3 + 3*x4\n\n## intercept, slope, & quadratic terms for Y\niY =~ 1*y1 + 1*y2 + 1*y3 + 1*y4\nsY =~ 0*y1 + 1*y2 + 2*y3 + 3*y4\nqY =~ 0*y1 + 1*y2 + 4*y3 + 9*y4\n\n## set variances \n\ny4 ~~ 2*y4\nx4 ~~ 1*x4\n\n## set latent means/intercepts\niX ~ 2*1\nsX ~ 1*1\nsY ~ -1*1\nqY ~ -1.5*1\n\nsY ~ 2*predictor\n\noutcome ~ 2*iX + 3*sY \n  '\n\n\n\nIn the data I simulated, Variable {x} is changing linearly over time and variable {y} is changing quadratically, which his what we found.\n\n\nshow code\n\nsim_growth_dat <- simulateData(\n  model = growth_mod, \n  model.type = \"growth\", \n  seed = 82020, \n  orthogonal = F,\n  auto.cov.y = T, \n  auto.var = T\n  )\n\n\n\n\n\n\n",
    "preview": "blog/growth-ppm/growth-ppm_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-08-03T14:30:09-07:00",
    "input_file": {}
  },
  {
    "path": "blog/decision-trees-in-r/",
    "title": "An introduction to `{Tidymodels}`: Decision Tree Learning in R",
    "description": "Exploring Decision Tree Variants in `{tidymodels}`.",
    "author": [
      {
        "name": "Cianna Bedford-Petersen",
        "url": {}
      },
      {
        "name": "Christopher Loan",
        "url": {}
      },
      {
        "name": "Brendan Cullen",
        "url": {}
      }
    ],
    "date": "2020-06-02",
    "categories": [
      "Machine Learning",
      "Decision Trees",
      "Model Comparisons",
      "Plotting",
      "Public Data",
      "Collaboration",
      "Tutorial"
    ],
    "contents": "\n\nContents\nIntro\nSetup\nImport the data\nExplore the data\nSplit data and resample\nPre-processing\nCreate a model\nThe type of model\nThe engine\nThe mode\nThe arguments\n\nCreate a workflow\nModel Examples\nBagged trees\nSpecify model\nCreate workflow\nFit the model\nVisualize\n\nRandom forest\nSpecify the model\nCreate workflow\nFit the model\nVisualize\n\nBoosted trees\nSpecify the model\nCreate workflow\nFit the model\nVisualize\n\n\nEvaluate metrics\nOut-of-sample performance\n\n\n\nshow code\n\nknitr::include_graphics(here::here(\"images\", \"tree.jpeg\"))\n\n\n\n\nIntro\nTidyverse’s newest release has recently come together to form a cohesive suite of packages for modeling and machine learning, called {tidymodels}. The successor to Max Kuhn’s {caret} package, {tidymodels} allows for a tidy approach to your data from start to finish. We’re going to walk through the basics for getting off the ground with {tidymodels} and demonstrate its application to three different tree-based methods for predicting student test scores. For further information about the package, you can visit https://www.tidymodels.org/.\nI’d also like to add here that I couldn’t have done this post without collaboration with other UO doctoral students Cianna Bedford-Petersen & Brendan Cullen (Psychology PhD program, specifically). Last I checked, they also hosted this exact tutorial on Brendan’s and Cianna’s websites, so if you liked this, check out some of their work.\nSetup\n\n\nshow code\n\noptions(scipen = 17)\n\n\n\nLoad both the {tidyverse} and {tidymodels} packages into your environment. We’ll also load in the {skimr} package to help us with some descriptives for our data and a host of other packages that will be required to run our machine learning models.\n\n\nshow code\n\nlibrary(tidymodels) \nlibrary(tidyverse) # manipulating data\nlibrary(skimr) # data visualization\nlibrary(baguette) # bagged trees\nlibrary(future) # parallel processing & decrease computation time\nlibrary(xgboost) # boosted trees\nlibrary(ggthemes)\ntheme_set(theme_economist())\n\n\n\nImport the data\nWe use simulated data which approximates reading and math scores for ~189,000 3rd-8th grade students in Oregon public schools see this Kaggle page for details. For the purpose of demonstration, we’ll be sampling 1% of the data with sample_frac() to keep computer processing time manageable. All school IDs in the data are real, so we can use that information to link the data with other sources. Specifically, we’re also going to pull in some data on student enrollment in free and reduced lunch from the National Center for Education Statistics and some ethnicity data from the Oregon Department of Education.\n\n\nshow code\n\nset.seed(100)\n# import data and perform initial cleaning\n# initial cleaning steps include: \n# *recode NA's for lang_cd and ayp_lep to more meaningful values\n# *remove vars with entirely missing data\n# Note: the data is called 'train.csv', but we will actually further split this into its own training and testing data\ndat <- read_csv(\"train.csv\") %>% \n  select(-classification) %>% # remove this variable because it's redundant with `score`\n  mutate(lang_cd = ifelse(is.na(lang_cd), \"E\", lang_cd), \n         ayp_lep = ifelse(is.na(ayp_lep), \"G\", ayp_lep)) %>% \n  sample_frac(.01) %>% # sample 1% of the data to reduce run time\n  janitor::remove_empty(c(\"rows\", \"cols\")) %>% \n  drop_na() %>% \n  select_if(~length(unique(.x)) > 1)\n# import fall membership report ethcnicity data and do some basic cleaning and renaming\nsheets <- readxl::excel_sheets(\"fallmembershipreport_20192020.xlsx\")\node_schools <- readxl::read_xlsx(\"fallmembershipreport_20192020.xlsx\",\n                                 sheet = sheets[4])\nethnicities <- ode_schools %>%\n  select(attnd_schl_inst_id = `Attending School ID`,\n         attnd_dist_inst_id = `Attending District Institution ID`,\n         sch_name = `School Name`,\n         contains(\"%\")) %>%\n  janitor::clean_names()\nnames(ethnicities) <- gsub(\"x2019_20_percent\", \"p\", names(ethnicities))\n# join ethnicity data with original dataset\ndat <- left_join(dat, ethnicities)\n# import and tidy free and reduced lunch data \nfrl <- rio::import(\"https://nces.ed.gov/ccd/Data/zip/ccd_sch_033_1718_l_1a_083118.zip\",\n              setclass = \"tbl_df\")  %>% \n  janitor::clean_names()  %>% \n  filter(st == \"OR\")  %>%\n  select(ncessch, lunch_program, student_count)  %>% \n  mutate(student_count = replace_na(student_count, 0))  %>% \n  pivot_wider(names_from = lunch_program,\n              values_from = student_count)  %>% \n  janitor::clean_names()  %>% \n  mutate(ncessch = as.double(ncessch))\n# import student counts for each school across grades\nstu_counts <- rio::import(\"https://github.com/datalorax/ach-gap-variability/raw/master/data/achievement-gaps-geocoded.csv\", setclass = \"tbl_df\")  %>% \n  filter(state == \"OR\" & year == 1718)  %>% \n  count(ncessch, wt = n)  %>% \n  mutate(ncessch = as.double(ncessch))\n# join frl and stu_counts data\nfrl <- left_join(frl, stu_counts)\n# add frl data to train data\ndat <- left_join(dat, frl)\n\n\n\nAfter loading in our three datasets, we’ll join them together to make one cohesive data set to use for modelling. After joining, the data contains both student-level variables (e.g. gender, ethnicity, enrollment in special education/talented and gifted programs, etc.) and district-level variables (e.g. school longitude and latitude, proportion of students who qualify for free and reduced-price lunch, etc.), all of which will be included for each 3 of our {tidymodels} tree-based examples.\nFor a more complete description of the variables, you can download the data dictionary here.\nExplore the data\nWe’ll use the skim() function from {skimr} to take a closer look at our variables. Many numeric predictors are clearly non-normal (see histograms below), but this is no problem as tree-based methods are robust to non-normality.\n\n\nshow code\n\ndat %>% \n  select(-contains(\"id\"), -ncessch, -missing, -not_applicable) %>%  # remove ID and irrelevant variables\n  mutate(tst_dt = lubridate::as_date(lubridate::mdy_hms(tst_dt))) %>% # covert test date to date\n  modify_if(is.character, as.factor) %>%  # convert character vars to factors\n  skim() %>% \n  select(-starts_with(\"numeric.p\")) # remove quartiles\n\n\nTable 1: Data summary\nName\nPiped data\nNumber of rows\n1857\nNumber of columns\n41\n_______________________\n\nColumn type frequency:\n\nDate\n1\nfactor\n25\nnumeric\n15\n________________________\n\nGroup variables\nNone\nVariable type: Date\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\ntst_dt\n0\n1\n2018-03-16\n2018-06-07\n2018-05-18\n47\nVariable type: factor\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\ngndr\n0\n1\nFALSE\n2\nM: 939, F: 918\nethnic_cd\n0\n1\nFALSE\n7\nW: 1151, H: 458, M: 100, A: 79\ntst_bnch\n0\n1\nFALSE\n6\nG6: 343, 1B: 330, G4: 304, G7: 304\nmigrant_ed_fg\n0\n1\nFALSE\n2\nN: 1793, Y: 64\nind_ed_fg\n0\n1\nFALSE\n2\nN: 1842, Y: 15\nsp_ed_fg\n0\n1\nFALSE\n2\nN: 1614, Y: 243\ntag_ed_fg\n0\n1\nFALSE\n2\nN: 1759, Y: 98\necon_dsvntg\n0\n1\nFALSE\n2\nY: 1100, N: 757\nayp_lep\n0\n1\nFALSE\n10\nG: 1471, F: 164, Y: 72, E: 58\nstay_in_dist\n0\n1\nFALSE\n2\nY: 1811, N: 46\nstay_in_schl\n0\n1\nFALSE\n2\nY: 1803, N: 54\ndist_sped\n0\n1\nFALSE\n2\nN: 1846, Y: 11\ntrgt_assist_fg\n0\n1\nFALSE\n3\nN: 1773, Y: 83, y: 1\nayp_schl_partic\n0\n1\nFALSE\n2\nY: 1846, N: 11\nayp_dist_prfrm\n0\n1\nFALSE\n2\nY: 1803, N: 54\nayp_schl_prfrm\n0\n1\nFALSE\n2\nY: 1785, N: 72\nrc_schl_partic\n0\n1\nFALSE\n2\nY: 1846, N: 11\nrc_dist_prfrm\n0\n1\nFALSE\n2\nY: 1803, N: 54\nrc_schl_prfrm\n0\n1\nFALSE\n2\nY: 1785, N: 72\nlang_cd\n0\n1\nFALSE\n2\nE: 1815, S: 42\ntst_atmpt_fg\n0\n1\nFALSE\n2\nY: 1853, P: 4\ngrp_rpt_schl_partic\n0\n1\nFALSE\n2\nY: 1846, N: 11\ngrp_rpt_dist_prfrm\n0\n1\nFALSE\n2\nY: 1845, N: 12\ngrp_rpt_schl_prfrm\n0\n1\nFALSE\n2\nY: 1834, N: 23\nsch_name\n1\n1\nFALSE\n699\nHig: 14, Jud: 14, Hou: 13, Fiv: 11\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\nhist\nenrl_grd\n0\n1\n5.44\n1.69\n▇▃▅▃▃\nscore\n0\n1\n2495.34\n115.19\n▁▁▂▇▁\nlat\n0\n1\n44.79\n0.99\n▂▁▂▅▇\nlon\n0\n1\n-122.51\n1.16\n▅▇▁▁▁\np_american_indian_alaska_native\n1\n1\n0.01\n0.06\n▇▁▁▁▁\np_asian\n1\n1\n0.04\n0.07\n▇▁▁▁▁\np_native_hawaiian_pacific_islander\n1\n1\n0.01\n0.01\n▇▁▁▁▁\np_black_african_american\n1\n1\n0.02\n0.04\n▇▁▁▁▁\np_hispanic_latino\n1\n1\n0.25\n0.18\n▇▅▂▁▁\np_white\n1\n1\n0.60\n0.20\n▁▃▅▇▅\np_multiracial\n1\n1\n0.06\n0.03\n▇▆▁▁▁\nfree_lunch_qualified\n0\n1\n231.23\n147.55\n▇▇▃▁▁\nreduced_price_lunch_qualified\n0\n1\n39.86\n24.77\n▆▇▃▁▁\nno_category_codes\n0\n1\n271.09\n165.44\n▆▇▃▁▁\nn\n0\n1\n816.07\n536.55\n▇▃▂▁▁\n\nWhile most of our predictors are categorical, we can use {corrplot} to better visualize the relationships among the numeric variables.\n\n\nshow code\n\ndat %>% \n  select(-contains(\"id\"), -ncessch, -missing, -not_applicable) %>% \n  select_if(is.numeric) %>% \n  select(score, everything()) %>% \n  cor(use = \"complete.obs\") %>% \n  corrplot::corrplot()\n\n\n\n\nSplit data and resample\nThe first step of our analysis is to split our data into two separate sets: a “training” set and a “testing” set. The training set is used to train a model and, if desired, to adjust (i.e., “tune”) the model’s hyperparameters before evaluating its final performance on our test data. By allowing us to test a model on a new sample, we assess “out of sample” accuracy (i.e., unseen data-—what all predictive models are interested in) and limit overfitting to the training set. We can do this efficiently with the initial_split() function. This comes from the {rsample} package, which is part of the {tidymodels} package that we already loaded. Defaults put 75% of the data in the training set and 25% in the test set, but this can be adjusted with the prop argument. Then, we’ll extract the training data from our split object and assign it a name.\nTo further prevent over-fitting, we’ll resample our data using vfold_cv(). This function outputs k-fold cross-validated versions of our training data, where k = the number of times we resample (unsure why v- is used instead of k- here). By using k = 10 data sets, we get a better estimate of the model’s out-of-sample accuracy. On top of decreasing bias from over-fitting, this is essential when tuning hyperparameters (though we plan to apply defaults and not tune here, for brevity). Though our use of 10-fold cross validation is both frequently used and effective, it should be noted that other methods (e.g., bootstrap resampling) or other k-values are sometimes used to accomplish the same goal.\n\n\nshow code\n\n# split the data\nsplit <- initial_split(dat)\n# extract the training data\ntrain <- training(split)\n# resample the data with 10-fold cross-validation (10-fold by default)\ncv <- vfold_cv(train)\n\n\n\nPre-processing\nBefore we add in our data to the model, we’re going to set up an object that pre-processes our data. This is called a recipe. To create a recipe, you’ll first specify a formula for your model, indicating which variable is your outcome and which are your predictors. Using ~. here will indicate that we want to use all variables other than score as predictors. Then, we can specify a series of pre-processing steps for our data that directs our recipe to assign our variables a role or performs feature engineering steps. Pre-processing may be sound uncommon, but if you’ve ever used lm() (or several other R functions) you’ve done some of this by simply calling the function (e.g., automatic dummy-coding to handle categorical data). This is beneficial because it gives the analyst more control, despite adding complexity to the process.\nA complete list of possible pre-processing steps can be found here: https://www.tidymodels.org/find/recipes/\n\n\nshow code\n\nrec <- recipe(score ~ ., train) %>% \n  step_mutate(tst_dt = as.numeric(lubridate::mdy_hms(tst_dt))) %>% # convert `test date` variable to a date \n  update_role(contains(\"id\"), ncessch, new_role = \"id vars\") %>% # declare ID variables\n  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>% # remove variables with zero variances\n  step_novel(all_nominal()) %>% # prepares test data to handle previously unseen factor levels \n  step_unknown(all_nominal()) %>% # categorizes missing categorical data (NA's) as `unknown`\n  step_medianimpute(all_numeric(), -all_outcomes(), -has_role(\"id vars\"))  %>% # replaces missing numeric observations with the median\n  step_dummy(all_nominal(), -has_role(\"id vars\")) # dummy codes categorical variables\n\n\n\nCreate a model\nThe last step before bringing in our data is to specify our model. This will call upon functions from the {parsnip} package, which standardizes language for specifying a multitude of statistical models. There are a few core elements that you will need to specify for each model\nThe type of model\nThis indicates what type of model you choose to fit, each of which will be a different function. We’ll be focusing on decision tree methods using bag_tree(), random_forest(), and boost_tree(). A full list of models can be found here https://www.tidymodels.org/find/parsnip/\nThe engine\nset_engine() calls the package to support the model you specified above.\nThe mode\nset_mode() indicates the type of prediction you’d like to use in your model, you’ll choose between regression and classification. Since we are looking to predict student scores, which is a continuous predictor, we’ll be choosing regression.\nThe arguments\nset_args() allows you to set values for various parameters for your model, each model type will have a specific set of parameters that can be altered. For these parameters, you can either set a particular value or you can use the tune function to search for the optimal value of each parameter. Tuning requires a few extra steps, so we will leave the default arguments for clarity. For more information on tuning check out https://tune.tidymodels.org/.\nCreate a workflow\nUp to this point we’ve been setting up a lot of individual elements and now it is time to combine them to create a cohesive framework, called a workflow, so we can run our desired models. First, we’ll use the workflow() command and then we’ll pulling the recipe and model we already created. The next section shows three examples of specifying models and creating a workflow for different decision tree methods.\nModel Examples\nBagged trees\nA bagged tree approach creates multiple subsets of data from the training set which are randomly chosen with replacement. Each subset of data is used to train a given decision tree. In the end, we have an ensemble of different models. The predictions from all the different trees are averaged together, giving us a stronger prediction than one tree could independently.\nSpecify model\n\n\nshow code\n\nset.seed(100)\nmod_bag <- bag_tree() %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"rpart\", times = 10) # 10 bootstrap resamples\n\n\n\nCreate workflow\n\n\nshow code\n\nwflow_bag <- workflow() %>% \n  add_recipe(rec) %>%\n  add_model(mod_bag)\n\n\n\nFit the model\n\n\nshow code\n\nset.seed(100)\nplan(multisession)\nfit_bag <- fit_resamples(\n  wflow_bag,\n  cv,\n  metrics = metric_set(rmse, rsq),\n  control = control_resamples(verbose = TRUE,\n                              save_pred = TRUE,\n                              extract = function(x) extract_model(x)))\n\n\n\nVisualize\nThe plot below shows the root nodes from a bagged tree made of 100 trees (10 folds x 10 bootstrapped resamples). Root nodes are the 1st node in a decision tree, and they are determined by which variable best optimizes a loss function (e.g., minimizes mean square error [MSE] for continuous outcomes or Gini Index for categorical outcomes). Put roughly, the most common root nodes can be thought of as the most “important” predictors.\n\n\nshow code\n\n# extract roots\nbag_roots <-  function(x){\n  x %>% \n  select(.extracts) %>% \n  unnest(cols = c(.extracts)) %>% \n  mutate(models = map(.extracts,\n                  ~.x$model_df)) %>% \n  select(-.extracts) %>% \n  unnest(cols = c(models)) %>% \n  mutate(root = map_chr(model,\n                     ~as.character(.x$fit$frame[1, 1]))) %>%\n  select(root)  \n}\n# plot\nbag_roots(fit_bag) %>% \n  ggplot(mapping = aes(x = fct_rev(fct_infreq(root)))) + \n  geom_bar() + \n  coord_flip() + \n  labs(x = \"root\", y = \"count\") +\n  theme(axis.text.y = element_text(size = 9, angle = 30))\n\n\n\n\nRandom forest\nRandom forest is similar to bagged tree methodology but goes one step further. In addition to taking random subsets of data, the model also draws a random selection of features. Instead of utilizing all features, the random subset of features allows more predictors to be eligible root nodes. This is particularly useful for handling high dimensionality data (e.g., have more variables than participants/cases).\nSpecify the model\n\n\nshow code\n\nset.seed(100)\nmod_rf <-rand_forest() %>%\n  set_engine(\"ranger\",\n             num.threads = parallel::detectCores(), \n             importance = \"permutation\", \n             verbose = TRUE) %>% \n  set_mode(\"regression\") %>% \n  set_args(trees = 1000)\n\n\n\nCreate workflow\n\n\nshow code\n\nwflow_rf <- workflow() %>% \n  add_model(mod_rf) %>% \n  add_recipe(rec)\n\n\n\nFit the model\n\n\nshow code\n\nset.seed(100)\nplan(multisession)\nfit_rf <- fit_resamples(\n  wflow_rf,\n  cv,\n  metrics = metric_set(rmse, rsq),\n  control = control_resamples(verbose = TRUE,\n                              save_pred = TRUE,\n                              extract = function(x) x)\n)\n\n\n\nVisualize\nThe plot below shows the root nodes from a random forest with 1000 trees (specified using set_args(trees = 1000) in the parsnip model object).\n\n\nshow code\n\n# extract roots\nrf_tree_roots <- function(x){\n  map_chr(1:1000, \n           ~ranger::treeInfo(x, tree = .)[1, \"splitvarName\"])\n}\nrf_roots <- function(x){\n  x %>% \n  select(.extracts) %>% \n  unnest(cols = c(.extracts)) %>% \n  mutate(fit = map(.extracts,\n                   ~.x$fit$fit$fit),\n         oob_rmse = map_dbl(fit,\n                         ~sqrt(.x$prediction.error)),\n         roots = map(fit, \n                        ~rf_tree_roots(.))\n         ) %>% \n  select(roots) %>% \n  unnest(cols = c(roots))\n}\n# plot\nrf_roots(fit_rf) %>% \n  group_by(roots) %>% \n  count() %>% \n  arrange(desc(n)) %>% \n  filter(n > 75) %>% \n  ggplot(aes(fct_reorder(roots, n), n)) +\n           geom_col() + \n           coord_flip() + \n  labs(x = \"root\", y = \"count\") +\n  theme_economist(horizontal = F) +\n  theme(axis.text.y = element_text(size = 8)) \n\n\n\n\nBoosted trees\nBoosted trees, like bagged trees, are an ensemble model. Instead of applying successive models to resampled data and pooling estimates, boosted trees fit the next tree to the residuals (i.e., error term) of the prior tree. The goal is to minimize residual error through multiple trees, and is typically done with fairly “shallow” decision tree (i.e., 1-6 splits in each tree). Though each model is only slightly improving the error rate, the sequential use of many shallow trees makes computationally efficient (i.e. reduced run time) and highly accurate predictions.\nSpecify the model\n\n\nshow code\n\nmod_boost <- boost_tree() %>% \n  set_engine(\"xgboost\", nthreads = parallel::detectCores()) %>% \n  set_mode(\"regression\")\n\n\n\nCreate workflow\n\n\nshow code\n\nwflow_boost <- workflow() %>% \n  add_recipe(rec) %>% \n  add_model(mod_boost)\n\n\n\nFit the model\n\n\nshow code\n\nset.seed(100)\nplan(multisession)\nfit_boost <- fit_resamples(\n  wflow_boost, \n  cv,\n  metrics = metric_set(rmse, rsq),\n  control = control_resamples(verbose = TRUE,\n                              save_pred = TRUE)\n)\n\n\n\nVisualize\nOne of the few downfalls of {tidymodels} is its (current) inability to plot these tree-based models. For the past two models, it was simpler to extract root nodes and plot them, but their interpretation (as we’re fitting to residuals instead of data sets) are not straightforward. For that reason, we don’t have any pretty plots here. Instead, we’ll skip to evaluating the metrics of all models.\nEvaluate metrics\nAfter running these three models, it’s time to evaluate their performance. We can do this with tune::collect_metrics(). The table below shows the estimate of the out-of-sample performance for each of our 3 models.\n\n\nshow code\n\ncollect_metrics(fit_bag) %>% \n  bind_rows(collect_metrics(fit_rf)) %>%\n  bind_rows(collect_metrics(fit_boost)) %>% \n  filter(.metric == \"rmse\") %>% \n  mutate(model = c(\"bag\", \"rf\", \"boost\")) %>% \n  select(model, everything()) %>% \n  knitr::kable()\n\n\nmodel\n.metric\n.estimator\nmean\nn\nstd_err\nbag\nrmse\nstandard\n98.39324\n10\n2.618597\nrf\nrmse\nstandard\n96.16870\n10\n2.248261\nboost\nrmse\nstandard\n95.35888\n10\n2.764773\n\nHere, we are faced with a common problem in the machine learning world: choosing between models that perform similarly (see overlapping standard errors). Whether we would prefer random forests or bagged trees may depend on computational efficiency (i.e., time) or other factors. In practice, tuning several hyperparameters may have made one model clearly preferable over the others, but in our case - relying on all defaults - we would probably have similar performance with both models on a new data set and would prefer random forest or boosted tree models for their efficiency.\nOut-of-sample performance\nThe final step is to apply each trained model to our test data using last_fit().\n\n\nshow code\n\n# bagged trees\nfinal_fit_bag <- last_fit(\n  wflow_bag,\n  split = split\n)\n# random forest\nfinal_fit_rf <- last_fit(\n  wflow_rf,\n  split = split\n)\n# boosted trees\nfinal_fit_boost <- last_fit(\n  wflow_boost,\n  split = split\n)\n\n\n\nThe table below shows the actual out-of-sample performance for each of our 3 models.\n\n\nshow code\n\n# show performance on test data\ncollect_metrics(final_fit_bag) %>% \n  bind_rows(collect_metrics(final_fit_rf)) %>%\n  bind_rows(collect_metrics(final_fit_boost)) %>% \n  filter(.metric == \"rmse\") %>% \n  mutate(model = c(\"bag\", \"rf\", \"boost\")) %>% \n  select(model, everything()) %>% \n  knitr::kable()\n\n\nmodel\n.metric\n.estimator\n.estimate\nbag\nrmse\nstandard\n93.36504\nrf\nrmse\nstandard\n91.18114\nboost\nrmse\nstandard\n94.22609\n\nAfter applying our 3 trained models to the unseen test data, it looks like random forest is the winner since it has the lowest RMSE. In this example, we only used 1% of the data to train these models, which could make it difficult to meaningfully compare their performance. However, the random forest model also results in the best out-of-sample prediction (RMSE = 83.47) when using all of the available data, which we did for the Kaggle competition.\n\n\n\n",
    "preview": "blog/decision-trees-in-r/Decision_Trees_in_R_files/figure-html5/unnamed-chunk-16-1.png",
    "last_modified": "2021-08-03T14:29:08-07:00",
    "input_file": {}
  }
]
