---
title: "APA 2021 Supplemental Material"
description: 
  "Supplmental Material for the Application of Generalized Linear Mixed-Effects Model Regression Trees with Multi-Modal Data poster presented at APA 2021, co-authored with Matthew C. Graham, Edwin M. Zorilla, Idalis Villanueva Alarcon, Jenefer Husman, & Keith Zvoch"
author:
  - name: Christopher M. Loan
    url: {}
date: 2021-08-03
categories:
  - Collaboration
  - Conference
  - Machine Learning
  - Multi-Level Modeling
  - MLM
  - Plotting
  - Simulated Data
  - Tutorial 
output:
  distill::distill_article:
    self_contained: false
    code_folding: show code
    toc: true
draft: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = T)
```

# Background

The purpose of these supplemental materials is to walk users through a few models and technical details associated with the `{glmertree}` package and provide references/supplemental reading for our poster. Refer to the Table of Contents (or scroll down) if you're just here for references. 

For readability, code is included but can be shown/hidden by clicking `show code`.

## load libraries

```{r libs}
library(glmertree)
library(tidyverse)
library(ggparty)
```

## simulating data

For data privacy, I'm using simulated data to walk you through these models. If you don't care about the simulation and just want to see model specification, keep reading; you're in the right place! If you're interested in knowing how I simulated the data, check out [this blog post of mine](www.christopherloan.com/blog/simulating-2-level-data-for-apa-2021-supplemental/).

The short of it: I simulated N = 1065 cases â€” the same number of as the APA 2021 poster. However, I did not simulate random effects for items across participants. This simplifies the cross-classified design of our poster to a simpler two-level design that has n = 71 level 2 units, each with n = 15 level 1 units. Each level 2 unit has an outcome (`outcome`), two variables that are related to the outcome (`x` and `z`), and a variable that is *not* associated with the outcome (`w`). 

```{r functions, include = FALSE}
simulate_interaction_w_2_levels <-
  function(
    n, ## number level 1 units
    j, ## number level 2 units
    intercept_lv1, ## intercept at level 1
    main_x, ## main effect of x
    main_z, ## main effect of z
    interaction, ## interaction of x and z
    residual_var_sd_lv1, ## standard deviation of residuals at level 1
    random_int_mean_lv2, ## mean of random intercept at level 2
    random_int_sd_lv2, ## standard deviation of random intercept at level 2,
    start_seed = 123
  ){
    
    ## placeholder for level 2 outcomes
    outcomes_j <- vector("list", j)
    
    ## for each variable, make list 
    ## as long as level 2
    ## fill each element with a list of level 1 outcomes
    
    
    x <- vector("list", j) 
    z <- vector("list", j) 
    w <- vector("list", j) 
    
    ## School distribution (intercept at level 2)
    ## Standard deviation of random intercept at level 2
    set.seed(start_seed)
    
    a_j <- rnorm(j, random_int_mean_lv2, random_int_sd_lv2)
    
    for(i in 1:j) {
      
      ## make a level 1 predictor variable:
      ## set multiple seeds that change each iteration
      ## prevents identical cases with seed
      
      set.seed(start_seed+i)
      x[[i]] <- rnorm(n)
      set.seed(start_seed-i)
      z[[i]] <- rnorm(n)
      set.seed(-start_seed + i)
      outcomes_j[[i]] <- 
        rnorm(
          n, 
          intercept_lv1 + 
            interaction*z[[i]]*x[[i]] + 
            main_x*x[[i]] + 
            main_z*z[[i]] + 
            a_j[i], 
          ## standard deviation of residual variance
          residual_var_sd_lv1
        )
      set.seed(start_seed*197+197*i)
      w[[i]] <- rnorm(n)
    }
    
    outcomes_df <- 
      data.frame(
        id = rep(1:j, each = n),
        x = unlist(x),
        z = unlist(z),
        outcome = unlist(outcomes_j),
        w = unlist(w)
      )
    
    return(outcomes_df)
  }
```

```{r plot_theme, include=F}
project_theme <- 
  theme(
    axis.line = element_line(size = 3, lineend = 'round'), 
    legend.position = 'none',
    plot.title.position = 'plot', 
    panel.grid.minor = element_blank(),
    axis.title.x = element_text(size = 18),
    axis.text = element_text(size = 16),
    axis.title.y = element_text(size = 20),
    plot.title = element_text(size = 25),
    strip.text = element_text(size = 25),
  )
```

```{r simulate}
sim_dat <- 
  simulate_interaction_w_2_levels(
    n = 15, ## number level 1 units at each level 2
    j = 71, ## number level 2 units
    intercept_lv1 = 4.25, ## intercept at level 1
    main_x = 1.25, ## main effect of x
    main_z = 2.00, ## main effect of z
    interaction = 0.75, ## interaction of x and z
    residual_var_sd_lv1 = 2.00, ## standard deviation of residuals at level 1
    random_int_mean_lv2 = 0, ## mean of random intercept at level 2
    random_int_sd_lv2 = 1.00, ## standard deviation of random intercept at level 2,
    start_seed = 123 ## ensure you can reproduce 
  ) %>% 
  tibble()
```

Put formally, we're simulating: 

$$
\begin{aligned}
  \operatorname{outcome}_{i}  &\sim N \left(\mu, \sigma^2 \right) \\
    \mu &=\alpha_{j[i]} + \beta_{1}(\operatorname{x}) + \beta_{2}(\operatorname{z}) + \beta_{3}(\operatorname{w}) + \beta_{4}(\operatorname{x} \times \operatorname{z}) \\
    \alpha_{j}  &\sim N \left(\mu_{\alpha_{j}}, \sigma^2_{\alpha_{j}} \right)
    \text{, for id j = 1,} \dots \text{,J}
\end{aligned}
$$
where 

$$\mu = 4.25$$
$$\sigma = 2.00$$
$$\beta_{1} = 1.25$$ 
$$\beta_{2} = 2.00$$ 
$$\beta_{3} = 0$$ 
$$\beta_{4} = 0.75$$
$$J = 71$$
$$\mu_{\alpha_{j}} = 0$$
$$\sigma_{\alpha_{j}}= 1.00$$

# Fitting Generalized Linear Mixed Effects Model Regression (GLMM) Trees via `{glmertree}`

Specifying models with `{glmertree}` is similar to multilevel models (MLM) with `{lme4}`. However, rather than *adding* random effects with `{lme4}`, you specify the model as follows: `forumla = outcome ~ fixed effects | random effects | splitting variables`.

A second consideration when fitting generalized mixed effects regression (GLMM) trees is the model's use of parameter instability tests; therefore, we need to account for cluster covariances in this process, so you want to specify the `id` variable to the `cluster = id` argument as well as in the specified formula. As far as I know, the current implementation only allows a 2 level structure, even if the MLM 

## Model 1: Fully Exploratory

Let's run a very exploratory model here. This is everything but the kitchen sink: an intercept-only model, with random intercept for each level 2 unit, and allowing splits to be made by any of the variables in the data.

```{r tree 1}
tree_1 <- 
  lmertree(
    data = sim_dat, 
    formula = 
      outcome ~ 1 | (1 | id) | x + z + w, 
    cluster = id, 
  )
```

Let's look at plotted results first. Here are the random effects, presented as a caterpillar plot

```{r plot ranef 1}
plot(tree_1, which = 'ranef')
```

Here is the model presented as a decision tree. With the number of splits we observe here, there is too much information to be very informative about individual nodes (there was actually so much information that I had to use another package `{ggparty}` to render the plot on this website in a more careful way than the default `plot(tree_1, which = 'tree')` method. We can see, however, that the model is identifying ranges of `z` and `x` that have different intercepts. The model is never presenting `w` as a variable influencing splitting when there is so much more heterogeneity in levels of `z` and `x`. So that's good!

```{r plot tree 1, layout = "l-screen", fig.width = 64, fig.height=48}
#plot(tree_1, which = 'tree')
significance_level <- function(p_value) {
  if (p_value < 0.001) return(c("p_value < 0.001"))
  if (p_value < 0.01) return(c("p_value < 0.01"))
  if (p_value < 0.05) return(c("p_value < 0.05"))
  else return("")}


ggparty(tree_1$tree[1]) +
    geom_edge(size = 1.2) +
    geom_node_label(
      aes(label = splitvar),
      ids = "inner") +
  geom_node_plot(
      gglist = list(
        geom_boxplot(
          aes(y = outcome), 
          size = 2),
        theme_minimal(base_size = 30),
        theme(
          axis.title.y = element_blank(),
          axis.text.y = element_blank(),
          axis.line.y = element_line(size = 1.5),
          panel.grid.minor.y = element_blank()
          )),
      shared_axis_labels = TRUE)+
  geom_node_label(
    line_list = 
      list(
        aes(label = splitvar),
        aes(label = paste('N = ' , nodesize)),
        aes(label = significance_level(p.value))),
    line_gpar = list(
      list(size = 80),
      list(size = 50),
      list(size = 50)),
    ids = "inner") +
  geom_node_label(
    aes(
      label = paste0("N = ", nodesize)
      ),
    ids = "terminal", 
    line_gpar = list(list(size = 50))
    )
```

There are two ways we could begin working to make results which can be interpreted. 



```{r}
tree_0 <- 
  lmertree(
    data = nonlinear_dat, 
    formula = 
      outcome ~ x | (1 | id) | w + z + x, 
    cluster = id, 
  )


tree_00 <- 
  lmertree(
    data = nonlinear_dat, 
    formula = 
      outcome ~ x | (1 | id) | w + z + x, 
    cluster = id, 
    maxdepth = 3
  )
summary(tree_00$tree)
```

```{r plot tree 1, layout = "l-body", fig.width = 64, fig.height=48}

significance_level <- function(p_value) {
  if (p_value < 0.001) return(c("p_value < 0.001"))
  if (p_value < 0.01) return(c("p_value < 0.01"))
  if (p_value < 0.05) return(c("p_value < 0.05"))
  else return("")}


ggparty(tree_00$tree[1]) +
    geom_edge(size = 1.2) +
  geom_edge_label() +
    geom_node_label(
      aes(label = splitvar),
      ids = "inner") +
  geom_node_plot(
      gglist = list(
        geom_point(
          aes(y = outcome, x = x), 
          size = 2),
        geom_smooth(method = 'lm', aes(y = outcome, x = x)),
        theme_minimal(base_size = 30),
        theme(
          axis.title.y = element_blank(),
          axis.text.y = element_blank(),
          axis.line.y = element_line(size = 1.5),
          panel.grid.minor.y = element_blank()
          )),
      shared_axis_labels = TRUE)+
  geom_node_label(
    line_list = 
      list(
        aes(label = splitvar),
        aes(label = paste('N = ' , nodesize)),
        aes(label = significance_level(p.value))),
    line_gpar = list(
      list(size = 20),
      list(size = 20),
      list(size = 20)),
    ids = "inner") +
  geom_node_label(
    aes(
      label = paste0("N = ", nodesize)
      ),
    ids = "terminal", 
    line_gpar = list(list(size = 20))
    )
```



### Add hyperparameters

Hyperparameters are parameters that can be specified by the researcher; these parameters influence the estimation of the parameters in the model. These are common for machine learning, and those relevant to GLMM trees are any that are relevant to decision trees or parameter instability tests (see [`{partykit}`](https://cran.r-project.org/web/packages/partykit/partykit.pdf)). 

I'll point you towards a few and their defaults.

* `bonferroni = TRUE`: Bonferroni corrects *p*-values. Recommended to leave on.
* `alpha = 0.05`: *p*-value indicated significant parameter stability. This is typically the first parameter I change, making the threshold more stringent (e.g., `alpha = 0.001`).
* `minsize = NULL`: the minimum cases allowed in the terminal node. The larger you make this, the more of the sample has to be in a node for you to trust there is generalize-able heterogeneity and not just noise in your data (e.g., `minsize = 0.10*nrow(your_data_set)`)
* `trim = NULL`: this specifies the percentage (if < 0) or number (if > 0) of outliers to be be omitted from the parameter instability tests. This is done to prevent undue influence of outliers, and does not influence group assignment/splitting; it is just used in testing.
* `maxdepth = Inf`: the depth of the tree that's allowed to grow. This functionally decreases the level of interactions you allow the data to find.

These should be tuned carefully and resulting models are ideally discussed with content experts and compared to prior research for validity. 

Let's re-run this with some more stringent hyperparameters and see if `x` and `z` are still contributing:



```{r tree 2}
tree_2 <- 
  lmertree(
    data = dat2, 
    formula = 
      outcome ~ 1 | (1 | id) | x + z + w + a + b + c + d, 
    cluster = id, 
    alpha = 0.001, 
    minsize = 0.15*nrow(sim_dat), 
    trim = 0.1, 
    #mtry = 3
  )
plot(tree_2$tree)

dat2 <- 
  sim_dat %>% 
  mutate(
    a = rnorm(nrow(sim_dat)),
    b = rnorm(nrow(sim_dat)),
    c = rnorm(nrow(sim_dat)), 
    d = rnorm(nrow(sim_dat))
  )
```

```{r plot tree 2, fig.width = 8, fig.height = 6}
plot(tree_2, which = 'tree')
```

These groups do not seem too small to interpret the effects, but we in a world so comfortable with inferential statistics, we could estimate fixed effects for these variables and see if we can still find subgroup effects. 

### Add Fixed Effects

```{r tree 3}
tree_3 <- 
  lmertree(
    data = sim_dat, 
    formula = 
      outcome ~ x | (1 | id) | w + z + x, 
    cluster = id, 
    alpha = 0.001, 
    minsize = 0.15*nrow(sim_dat), 
    trim = 0.1
  )
```

```{r plot tree 3, fig.width = 8, fig.height = 6}
plot(tree_3, which = 'tree')
```

This figure suggests the association of `x` varies by level of `z`, such that higher levels of `z` correspond to greater average levels of `outcome` and greater associations between `x` and `outcome` (i.e., higher intercepts and slopes).

In other words, when `z` increases, both `outcome` increases and the influence of `x` on `outcome` increases.  

Of course, you could then add the fixed effect of z, to control for it's influence.

```{r tree 4}
tree_4 <- 
  lmertree(
    data = sim_dat, 
    formula = 
      outcome ~ x + z | (1 | id) | w + z + x, 
    cluster = id, 
    alpha = 0.001, 
    minsize = 0.15*nrow(sim_dat), 
    trim = 0.1
  )
```

```{r plot tree 4, fig.width = 8, fig.height = 6}
plot(tree_4, which = 'tree')
```

You see, even controlling for level of `z`, the interaction is observed here. the relation of `z` to `outcome` is fairly constant. 

```{r}
summary(tree_4$tree)
```


```{r tree 5}
tree_5 <- 
  lmertree(
    data = sim_dat, 
    formula = 
      outcome ~ x * z | (1 | id) | w + z, 
    cluster = id, 
    alpha = 0.001, 
    minsize = 0.15*nrow(sim_dat), 
    trim = 0.1
  )
```

# MLMs via `{lme4}`

If we run a traditional multilevel model (MLM) on this data, we see how closely the MLM estimates identify the simulated effects. We can see the nuisance variable `w` correctly does not have significant associations with the outcome 

```{r lmer}
confirmation_lmer <- 
  lmer(
    data = sim_dat, 
    formula = outcome ~ x * z + w +
      (1 | id)
  )
summary(confirmation_lmer)
```

# References & Recommended Reading

Bates, D., Maechler, M., Bolker, B., & Walker, S. (2015). Fitting Linear Mixed-Effects Models Using lme4. Journal of Statistical Software, 67(1), 1-48. doi:10.18637/jss.v067.i01.

Fokkema, M., Edbrooke-Childs, J., & Wolpert, M. (2020). Generalized linear mixed-model (GLMM) trees: A flexible decision-tree method for multilevel and longitudinal data. Psychotherapy Research, 31(3), 329-341.

Fokkema, M., Smits, N., Zeileis, A., Hothorn, T., & Kelderman, H. (2018). Detecting treatment-subgroup interactions in clustered data with generalized linear mixed-effects model trees. Behavior research methods, 50(5), 2016-2034.

Hothorn, T. & Zeileis, A. (2015). partykit: A Modular Toolkit for Recursive Partytioning in R. Journal of Machine Learning Research, 16, 3905-3909. 

Hothorn, T., Hornik, K., & Zeileis, A. (2006). Unbiased recursive partitioning: A conditional inference framework. Journal of Computational and Graphical statistics, 15(3), 651-674.

Hulleman, C. S., Godes, O., Hendricks, B. L., & Harackiewicz, J. M. (2010). Enhancing interest and performance with a utility value intervention. Journal of educational psychology, 102(4), 880.

Borkovec, M. & Madin, N. (2019). ggparty: 'ggplot' Visualizations for the 'partykit' Package. R package version 1.0.0.

Villanueva, I., Husman, J., Christensen, D., Youmans, K., Khan, M.T., Vicioso, P., Lampkins, S. and Graham, M.C., 2019. A Cross-Disciplinary and Multi-Modal Experimental Design for Studying Near-Real-Time Authentic Examination Experiences. JoVE (Journal of Visualized Experiments), (151), p.e60037.

Wickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, 

Zeileis, A. (2001). strucchange: Testing for structural change in linear regression relationships. R News, 1(3), 8-11.

Zeileis, A., Hothorn, T., & Hornik, K. (2008). Model-based recursive partitioning. Journal of Computational and Graphical Statistics, 17(2), 492-514.